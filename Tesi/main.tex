% base
\documentclass{book}

% characters
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}

% math
\usepackage{amsmath}
\usepackage{mathtools}

% graphics
\usepackage{mathdots}
\usepackage{nicematrix}
\setcounter{MaxMatrixCols}{20}
\usepackage{rotating}
\definecolor{lblue}{rgb}{.80, .90, .95}

\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1} % red
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4} % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}  % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}    % green

% font
% \usepackage{mathpazo}
% \usepackage{eulervm}
\usepackage{amsfonts}

% code
\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean}
\usepackage{float}

% sections
\usepackage{emptypage}
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bfseries}{\thechapter.}{20pt}{\huge}

% theorems
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\theoremstyle{plain}
\newtheorem{proposition}{Proposition}

% hyphenation
\usepackage{hyphenat}
\hyphenation{pri-mi-tive}

% other
\usepackage[colorlinks=true]{hyperref}
\usepackage{csquotes}

% macro
% \newcommand{\svdots}{\scriptscriptstyle \boldsymbol \vdots}
\newcommand{\perm}[1]{\scriptstyle \left\rmoustache #1 \right\rmoustache}
\newcommand{\bloch}[2]{\Block[draw=white,fill=lblue,line-width=.5mm,rounded-corners]{#1}{#2}} % https://en.wikipedia.org/wiki/Ernest_Bloch
\newcommand{\conv}[1]{\overbracket[.5pt][1pt]{\underbracket[.5pt][1pt]{#1}}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\RPP}{\mathsf{RPP}}
\newcommand{\ORPP}{\mathsf{ORPP}}
\newcommand{\rppf}{\mathsf{f}}
\newcommand{\rppg}{\mathsf{g}}
\newcommand{\rpph}{\mathsf{h}}
\newcommand{\rppId}{\mathsf{Id}}
\newcommand{\rppNe}{\mathsf{Ne}}
\newcommand{\rppSu}{\mathsf{Su}}
\newcommand{\rppPr}{\mathsf{Pr}}
\newcommand{\rppSw}{\mathsf{Sw}}
\newcommand{\rppCo}{\fatsemi}
\newcommand{\rppPa}{\Vert}
\newcommand{\rppIt}{\mathsf{It}}
\newcommand{\rppIta}{\mathsf{Ita}}
\newcommand{\rppItr}{\mathsf{Itr}}
\newcommand{\rppIf}{\mathsf{If}}
\newcommand{\rppinc}{\mathsf{inc}}
\newcommand{\rppdec}{\mathsf{dec}}
\newcommand{\rppmul}{\mathsf{mul}}
\newcommand{\rppsquare}{\mathsf{square}}
\newcommand{\PRF}{\mathsf{PRF}}

% title
\title{A Formal Verification of Reversible Primitive Permutations}
\author{Giacomo Maletto}
\date{}

\begin{document}

\maketitle

\chapter{The definition}

\section{Reversible computing}

Reversible computing is a model of computation in which every process can be run backwards.
Simply put, in a reversible setting any program takes inputs and gives outputs (like usual), but can also go the other way around:
provided the output it can reconstruct the input.
In a mathematical sense, every function is expected to be invertible.

Why do we care about such a thing?

Firstly, having a programming language in which every function (or even a subset of functions) is reversible could lead to interesting and practical applications.

But we can also imagine reversible computers, in which the underlying architecture is inherently reversible:
Toffoli gates provide a way to do so.
The opposite of reversibility is loss of information, which (for thermodynamic reasons) leads to loss of energy and heat dissipation.
This means that a non-reversible gate dissipates energy each time information is discarded, while in principle a reversible computer wouldn't.

Lastly, reversible computing is directly related to quantum computing, as each operation in a quantum computer must be reversible.


\section{Reversible Primitive Permutations}

In the article we decided to formalize, the authors focus on providing a functional model of reversible computation.
They develop an inductively defined set of functions, called \textbf{Reversible Primitive Permutations} or \textbf{RPP},
which are expressive enough to represent all Primitive Recursive Functions -
that is to say, $\RPP$ is $\PRF$-complete (we talk about what this means in section ?).
Here is the definition that we will use:

\newpage

\begin{definition}[Reversible Primitive Permutations]
\label{rppdef}
The class of \textbf{Reversible Primitive Permutations} or $\RPP$ is the smallest subset of functions $\Z^n \to \Z^n$ satisfying the following conditions:
\begin{itemize}

\item
The $n$-ary \textbf{identity} $\rppId_n (x_1, \dots, x_n) = (x_1, \dots, x_n)$ belongs to $\RPP$, for all $n \in \N$.
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{3-1}{\rppId_n} & x_1    \\
  \Vdots &                       & \Vdots \\
  x_n    &                       & x_n    \\
\end{NiceMatrix}\]
The meaning of these diagrams should be fairly obvious:
if the values on the left of a function are provided as inputs to that function, we get the values on the right as outputs.
\item
The \textbf{sign-change} $\rppNe (x) = -x$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppNe} & -x \\
\end{NiceMatrix}\]

\item
The \textbf{successor function} $\rppSu (x) = x+1$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppSu} & x+1 \\
\end{NiceMatrix}\]

\item
The \textbf{predecessor function} $\rppPr (x) = x-1$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppPr} & x-1 \\
\end{NiceMatrix}\]

\item
The \textbf{swap} $\rppSw (x, y) = (y, x)$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{2-1}{\rppSw} & y \\
  y &                     & x \\
\end{NiceMatrix}\]

\item
If $f : \Z^n \to \Z^n$ and $g : \Z^n \to \Z^n$ belongs to $\RPP$,
then the \textbf{series composition} $(f \rppCo g) : \Z^n \to \Z^n$ belongs to $\RPP$ and is such that:
\[(f \rppCo g) (x_1, \dots, x_n) = g (f (x_1, \dots, x_n)) = (g \circ f) (x_1, \dots, x_n).\]
We remark that $f \rppCo g$ means that $f$ is applied first, and then $g$, in opposition to the standard functional composition (denoted by $\circ$).
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{3-1}{f \rppCo g} & z_1    & \Block{3-1}{=} & x_1    & \bloch{3-1}{f} & y_1    & \bloch{3-1}{g} & z_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots &                & \Vdots \\
  x_n    &                         & z_n    &                & x_n    &                & y_n    &                & z_n    \\
\end{NiceMatrix}\]

\item

If $f : \Z^n \to \Z^n$ and $g : \Z^m \to \Z^m$ belongs to $\RPP$,
then the \textbf{parallel composition} $(f \rppPa g) : \Z^{n + m} \to \Z^{n + m} $ belongs to $\RPP$ and is such that:
\[(f \rppPa g) (x_1, \dots, x_n, y_1, \dots, y_m) = (f (x_1, \dots, x_n), g (y_1, \dots, y_m)).\]
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{6-1}{f \rppPa g} & w_1    & \Block{6-1}{=} & x_1    & \bloch{3-1}{f} & w_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots \\
  x_n    &                         & w_n    &                & x_n    &                & w_n    \\
  y_1    &                         & z_1    &                & y_1    & \bloch{3-1}{g} & z_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots \\
  y_m    &                         & z_m    &                & y_m    &                & z_m    \\
\end{NiceMatrix}\]

\item
If $f : \Z^n \to \Z^n$ belongs to $\RPP$,
then then \textbf{finite iteration} $\rppIt[f] : \Z^{n + 1} \to \Z^{n + 1}$ belongs to $\RPP$ and is such that:
\[ \rppIt[f] (x, x_1, \dots, x_n) = (x, (\overbrace{f \circ \dots \circ f}^{\downarrow x \text{ times}}) (x_1, \dots, x_n)) \]
where $\downarrow (\cdot) : \Z \to \N$ is defined as
\[\downarrow x = \begin{cases} x, & \text{if $x \ge 0$} \\
                               0, & \text{if $x < 0$} \end{cases}.\]
This means that the function $f$ is applied $\downarrow x$ times to $(x_1, \dots, x_n)$.
\[\begin{NiceMatrix}[nullify-dots]
  x      & \bloch{4-1}{\rppIt[f]} & x      & \Block{4-1}{=} & x      &                                                                       &                    &                & x      \\  
  x_1    &                        & y_1    &                & x_1    & \bloch{3-1}{f}                                                        & \Block{3-1}{\dots} & \bloch{3-1}{f} & y_1    \\
  \Vdots &                        & \Vdots &                & \Vdots &                                                                       &                    &                & \Vdots \\
  x_n    &                        & y_n    &                & x_n    &                                                                       &                    &                & y_n    \\
         &                        &        &                &        & \Block{1-3}{\underbrace{\hspace{5.5em}}_{\downarrow x \text{ times}}} &                    &                &        \\
\end{NiceMatrix}\]

\item
If $f, g, h : \Z^n \to \Z^n$ belong to $\RPP$,
then the \textbf{selection} $\rppIf[f, g, h] : \Z^{n + 1} \to \Z^{n + 1}$ belongs to $\RPP$ and is such that:
\[\rppIf[f, g, h] (x, x_1, \dots, x_n) = \begin{cases} (x, f (x_1, \dots, x_n)), & \text{if $x > 0$} \\
                                                    (x, g (x_1, \dots, x_n)), & \text{if $x = 0$} \\
                                                    (x, h (x_1, \dots, x_n)), & \text{if $x < 0$} \end{cases}.\]
We remark that the argument $x$ which determines which among $f$, $g$ and $h$ must be used cannot be among the arguments of $f$, $g$ and $h$,
as that would break reversibility.
\[\begin{NiceMatrix}[nullify-dots]
  x      & \bloch{4-1}{\rppIf[f, g, h]} & x      \\
  x_1    &                              & y_1    \\
  \Vdots &                              & \Vdots \\
  x_n    &                              & y_n    \\
\end{NiceMatrix} \quad
\begin{NiceMatrix}[nullify-dots]
                 &                     &                  \\
  \Block{3-1}{=} & f (x_1, \dots, x_n) & \text{if } x > 0 \\
                 & g (x_1, \dots, x_n) & \text{if } x = 0 \\
                 & h (x_1, \dots, x_n) & \text{if } x < 0 \\
\CodeAfter
\SubMatrix\}{2-1}{4-1}\{  
\end{NiceMatrix}\]

\end{itemize}
\end{definition}

\begin{remark}
\label{different_arity}
If we have two functions of different arity, for example $f : \Z^3 \to \Z^3$ and $g : \Z^5 \to \Z^5$,
then we will still write $f \rppCo g$ to mean the function with arity $\max(3, 5)=5$ given by $(f \rppPa \rppId_2) \rppCo g$.
In general, the arity of the "smaller" function can be enlarged by a suitable parallel composition with the identity.
The same goes for the arguments of the selection $\rppIf[f, g, h]$.
\[\begin{NiceMatrix}[nullify-dots]
  x_1 & \bloch{5-1}{f \rppCo g} & z_1 & \Block{5-1}{=} & x_1 & \bloch{3-1}{f} & y_1 & \bloch{5-1}{g} & z_1 & \Block{5-1}{=} & x_1 & \bloch{3-1}{f}        & y_1 & \bloch{5-1}{g} & z_1 \\
  x_2 &                         & z_2 &                & x_2 &                & y_2 &                & z_2 &                & x_2 &                       & y_2 &                & z_2 \\
  x_3 &                         & z_3 &                & x_3 &                & y_3 &                & z_3 &                & x_3 &                       & y_3 &                & z_3 \\
  x_4 &                         & z_4 &                & x_4 &                & x_4 &                & z_4 &                & x_4 & \bloch{2-1}{\rppId_2} & x_4 &                & z_4 \\
  x_5 &                         & z_5 &                & x_5 &                & x_5 &                & z_5 &                & x_5 &                       & x_5 &                & z_5 \\
\end{NiceMatrix}\]
\end{remark}


\section{Some examples}

In order to get accustomed to this definition, let's see some examples.

\paragraph{Increment and decrement}
Let's try to imagine what addition should look like in $\RPP$.
Of course, addition is usually thought of as a function which takes two inputs and yields their sum:
something like $\texttt{add}(x,y) = x+y$.
But notice that this operation is not reversible:
given only the output (the value $x+y$) it is impossible to obtain the original values ($x, y$).
As we will see, every function in $\RPP$ is reversible, so we will not be able to define addition in this way.

Instead, we can define a function $\rppinc$ in $\RPP$ which, given $n \in \N$ and $x \in \Z$, yields
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppinc} & n     \\
  x &                      & x + n \\
\end{NiceMatrix}\]
If $n$ is negative the output is just $(n, x)$.
The fact that the above diagram is only valid for $n \in \N$ might bother some of you;
we'll explain later why it is so, and how we can also make it work for $n \in \Z$.

For now let's focus on the output: we don't just have $x + n$ but also $n$, and indeed,
given both $n$ and $x+n$ we can reconstruct $n$ (obviously) and $x$ (by $(x+n)-n$).
As a matter of fact, the following function $\rppdec$ also belongs to $\RPP$:
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppdec} & n     \\
  x &                      & x - n \\
\end{NiceMatrix}\]
and if we try to compose $\rppinc$ and $\rppdec$ we get this remarkable result:
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppinc} & n     & \bloch{2-1}{\rppdec} & n \\
  x &                      & x + n &                      & x \\
\end{NiceMatrix}\]
and similarly for $\rppdec \rppCo \rppinc$.
So indeed $\rppdec$ is the inverse of $\rppinc$, and we can write $\rppdec = \rppinc^{-1}$.

But we haven't said how to actually define $\rppinc$.
Well, just like this:
\[\rppinc = \rppIt[\rppSu]\]
This means that we apply the successor function $\rppSu$ to the value $x$, for $\downarrow n$ times.
If $n \in \N$ then $\downarrow n = n$, so we effectively add $n$ to the value $x$.
If instead $n$ is negative then $\downarrow n = 0$ and nothing changes.

Can you guess how $\rppdec$ is defined?
\newpage
In a very similar manner, using the predecessor function:
\[\rppdec = \rppIt[\rppPr]\]
and as we will shortly see, finding the inverse is not something that we have to do by hand.

\paragraph{Multiplication and square}
We now turn our attention to multiplication.
The elementary-school way to define multiplication is by repeated addition, and we can define $\rppmul$ exactly like that:
\[\rppmul = \rppIt[\rppinc].\]
As $\rppinc$ had arity $2$, $\rppmul$ has arity $2+1=3$.
If $n, m \in \N$ and $x \in \Z$ then we have
\[\begin{NiceMatrix}
  n & \bloch{3-1}{\rppmul} & n             \\
  m &                      & m             \\
  x &                      & x + n \cdot m \\
\end{NiceMatrix}\]
because we're essentially "incrementing by $m$" $n$ times;
so in this case we preserve both inputs and increase a certain variable $x$.

What is the inverse $\rppmul^{-1}$? Does it perform division?
Well, the truth is rather disappointing:
\[\begin{NiceMatrix}
  n & \bloch{3-1}{\rppmul^{-1}} & n             \\
  m &                           & m             \\
  x &                           & x - n \cdot m \\
\end{NiceMatrix}\]
We will see a way to calculate division in $\RPP$, but this is not it.

We're now ready to define the function $\rppsquare$ which is used to calculate the square of a number:
\[\rppsquare = (\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw).\]
That might look like a very complicated expression;
thankfully we can make use of diagrams to show what each step does.
Given $n \in \N$ and $x \in \Z$ we have
\[\begin{NiceMatrix}
  n &                     & n & \bloch{2-1}{\rppinc} & n & \bloch{3-1}{\rppmul} & n             & \bloch{2-1}{\rppdec} & n             &                     & n             \\
  x & \bloch{2-1}{\rppSw} & 0 &                      & n &                      & n             &                      & 0             & \bloch{2-1}{\rppSw} & x + n \cdot n \\
  0 &                     & x &                      & x &                      & x + n \cdot n &                      & x + n \cdot n &                     & 0             \\
\end{NiceMatrix}\]
so we add the result $n \cdot n$ to a variable $x$;
we also require an additional value initialized to 0.
We will make frequent use of variables initially set to 0 and which come back to 0 after the calculation;
these are traditionally called \textbf{ancillary arguments} or \textbf{ancillaes}, from the latin term used to describe female house slaves in ancient Rome.

You might be wondering what would happen if $n < 0$ or the ancilla was different from $0$.
The truth is, we don't really care.
We will often specify the behaviour of these functions given some initial values,
and we won't need to know what happens for different initial values because we'll never use those functions in other ways.

\section{Calculating the inverse}

Earlier we hinted at the fact that every function in $\RPP$ is invertible and the inverse belongs to $\RPP$;
furthermore, we don't need to perform the calculation manually, case by case.
In other words, there is an \textit{effective procedure} which produces the inverse $f^{-1} \in \RPP$ given any $f \in \RPP$.

\begin{proposition}[The inverse $f^{-1}$ of any $f$]
\label{rppinv}
Let $f : \Z^n \to \Z^n$ belong to $\RPP$.
Then the inverse $f^{-1} : \Z^n \to \Z^n$ exists, belongs to $\RPP$ and, by definition, is
\begin{itemize}
\item $\rppId_n^{-1} = \rppId_n$
\item $\rppNe^{-1} = \rppNe$
\item $\rppSu^{-1} = \rppPr$
\item $\rppPr^{-1} = \rppSu$
\item $\rppSw^{-1} = \rppSw$
\item $(f \rppCo g)^{-1} = g^{-1} \rppCo f^{-1}$
\item $(f \rppPa g)^{-1} = f^{-1} \rppPa g^{-1}$
\item ${\rppIt[f]}^{-1} = \rppIt[f^{-1}]$
\item ${\rppIf[f, g, h]}^{-1} = \rppIf [f^{-1}, g^{-1}, h^{-1}]$
\end{itemize}
Then $f \rppCo f^{-1} = \rppId_n$ and $f^{-1} \rppCo f = \rppId_n$.
\end{proposition}
\begin{proof}
By induction on the definition of $f$.
\end{proof}

Well, that was rather succint.

We invite the reader to check that every listed inverse does indeed make sense;
for example, the function $\rppIt[f] (x, y_1, \dots, y_n)$ applies $\downarrow x$ times the function $f$ to the argument $(y_1, \dots, y_n)$.
If we want to "undo" this effect we just need to apply $\downarrow x$ times $f^{-1}$ to the same argument, so
${\rppIt[f]}^{-1} = \rppIt[f^{-1}]$.

Of course, that reasoning only works if in turn $f$ is also invertible and $f^{-1} \in \RPP$.
This is the reason that the proof is by induction:
given an arbitrary $\RPP$, if we unfold one step of the definition we get one of the cases listed.
We apply the appropriate step and then by inductive hypothesis we can assume that in turn its sub-terms are invertible.

Notice that in this way, if we try to calculate $\rppinc^{-1}$ we really do get $\rppdec$, as we had hoped for.

Since $\RPP$ is inductively defined, any proposition involving $\RPP$ functions can be proven using induction.
Not only that, but any function which has for an argument a generic $\RPP$ can be defined recursively,
and indeed we can also see $(\cdot)^{-1}:\RPP \to \RPP$ as a recursive function.
Now that we delve into the Lean theorem prover we will see that induction and recursion can be seen as really the same thing,
and that's just one of many similarities between functions and proofs.

\section{First steps with Lean}

In this section we take a look at some of Lean's basic features.
You don't have to understand every detail -
just enough to have a vague sense of what it's like to define stuff in Lean.

\paragraph{}

In Lean we primarily do three things:
\begin{enumerate}
\item define data structures
\item define functions
\item prove theorems about data structures and functions
\end{enumerate}
What sets Lean apart from your average functional programming language (like Haskell) is the third item on the list.
Now we will instead focus on the first and second points.

You don't have to understand every detail of what will follow -
a vague understanding of what's going on would be sufficient.
The curious reader can run and play with most of the following snippets of code
in the online editor \url{https://leanprover-community.github.io/lean-web-editor/}.

\paragraph{A simple example of a type}
Data is defined using the \lstinline{inductive} keyword.
Here is the typical example of data structure:
\begin{lstlisting}
inductive weekday : Type
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday
| sunday : weekday
\end{lstlisting}
This defines a \textit{type} called \lstinline{weekday}.
Days of the week like \lstinline{monday}, \lstinline{tuesday}, etc. are elements of the type \lstinline{weekday}.
We can see the type of an element by using the \lstinline{#check} command:
\begin{lstlisting}
-- opening the scope weekday (otherwise to refer
-- to an element - for example tuesday - of weekday
-- we have to write weekday.tuesday)
open weekday

#check tuesday -- this outputs "weekday"
\end{lstlisting}
and we write this as
\begin{center}\begin{lstlisting}
tuesday : weekday
\end{lstlisting}\end{center}
Everything in Lean has a type (and only one). For example, natural numbers have type \lstinline{ℕ}:
\begin{lstlisting}
#check 3 -- ℕ
\end{lstlisting}
Even type themselves have a type\footnote{Types in Lean have a role similar to sets in math.
Standard math axioms (like ZFC) dictates that everything is a set, including sets themselves.
This basic notion can lead to some contradictory statements, like the famous Russell's paradox
(let's consider the set of all sets that do not contain themselves; does this set contain itself?)
and if one is not careful in defining types of types, the same thing could happen with type theory.
But in fact, type theory was invented in the beginning of the 20th century by Bertrand Russell precisely to avoid Russell's paradox.
The approach used in Lean is to define a cumulative hierarchy of universes \lstinline{Type : Type 1 : Type 2 ...},
so that it's impossible to invoke objects like "the type of all types" or a type having itself as an element.}.
Lean's type system is very expressive, and makes it possible to work with complex math in Lean.

We can define functions over the type \lstinline{weekday} -
for example, the function \lstinline{next}:

\begin{lstlisting}
-- Special characters like → will abound.
-- In VS Code and the Lean Web Editor,
-- arrows can be inserted by typing \to and hitting
-- the space bar. 
def next : weekday → weekday
| monday    := tuesday
| tuesday   := wednesday
| wednesday := thursday
| thursday  := friday
| friday    := saturday
| saturday  := sunday
| sunday    := monday

#reduce next wednesday -- this outputs "thursday"
#check next -- next has type "weekday → weekday"
\end{lstlisting}
This function is defined by cases:
if we have \lstinline{monday}, output \lstinline{tuesday},
if we have \lstinline{tuesday}, output \lstinline{wednesday}, and so on.

(Almost) every expression - like \lstinline{next (next thursday)} or \lstinline{3 * 5 + 2} -
have a corresponding \textit{reduced form} (respectively \lstinline{saturday} and \lstinline{17})
which can be displayed using the \lstinline{#reduce} command,
and is obtained by repeatedly applying functions to their arguments, until the full computation is carried out.
In this sense, things like \lstinline{next wednesday} and \lstinline{next (next tuesday)}
(or \lstinline{2 + 2} and \lstinline{1 + 3}) are \textit{definitionally equivalent}, because they're reduced to the same expression.

An important remark on notation:
in math it is customary to call functions by enclosing arguments in parenthesis and separating them with commas, i.e. $f(x,y,z)$.
Languages like Lean follow a different convention: the arguments are simply written after the function name, like \lstinline{f x y z}.
So in our case, what we would write as $\mathsf{next(next(thursday))}$ is instead written \lstinline{next (next thursday)}
(writing \lstinline{next next thursday} would be wrong
because it would mean that the first argument to \lstinline{next} is the function \lstinline{next} itself, not \lstinline{next thursday}).
This leads to no ambiguity and often helps reducing clutter.

\paragraph{An example of an inductive type}
Right now you could be wondering why we used the keyword \lstinline{inductive} to define \lstinline{weekday},
when there's \textit{clearly} no induction going on at all in its construction.
First of all, it depends on what you mean by induction; but it is true that that was a particularly simple case.
As an example of a more overtly inductive object, we can define the natural numbers like this:
\begin{lstlisting}
inductive Nat : Type
| Zero : Nat
| Succ (n : Nat) : Nat
\end{lstlisting}
The name \lstinline{Nat} and subsequent objects are capitalized in order to avoid conflict with the definition of \lstinline{nat} already present in Lean.
We can read this definition as "every element of the type \lstinline{Nat} is either \lstinline{Zero}
or \lstinline{Succ n} where \lstinline{n : Nat}",
which is basically the Peano definition of natural numbers.
Some examples of elements of this type:
\begin{lstlisting}
open Nat

-- all these outputs "Nat"
#check Zero -- represents 0
#check Succ Zero -- represents 1
#check Succ (Succ Zero) -- represents 2
#check Succ (Succ (Succ Zero)) -- represent 3
#check Zero.Succ.Succ.Succ -- also represents 3
                           -- alternative notation
\end{lstlisting}
Functions over \lstinline{Nat} have the possibility of being truly recursive:
for example, we can recursively define addition \lstinline{Add m n} by induction over \lstinline{n}.
\begin{itemize}
\item if \lstinline{n = Zero} then \lstinline{Add m Zero = m}
\item if \lstinline{n = Succ n'} for some \lstinline{n' : Nat}, \\
      then \lstinline{Add m (Succ n') = Succ (Add m n')}.
\end{itemize}
Note that by definition each element of \lstinline{Nat} can be either \lstinline{Zero} or \lstinline{Succ n'} for some \lstinline{n' : Nat},
so the two cases considered cover all possibilities.
Written in Lean,
\begin{lstlisting}
def Add : Nat → Nat → Nat
| m Zero      := m
| m (Succ n') := Succ (Add m n')
\end{lstlisting}
It may have struck you that the type of \lstinline{Add} is not \lstinline{Nat × Nat → Nat}
but instead \lstinline{Nat → Nat → Nat}.
This is known as currying, and it's not as strange as it might look like at first.
Consider this: we can think of \lstinline{Add} as a function which takes a pair \lstinline{(m,n) : Nat × Nat} and outputs \lstinline{Add m n : Nat},
as is standard in mathematics.
But we can also think of it as a function which takes just \lstinline{m : Nat} and outputs the function \lstinline{Add m : Nat → Nat},
which in turn given \lstinline{n : Nat} outputs \lstinline{Add m n : Nat}.
We can think of \lstinline{Add m} as a partially applied function, which becomes fully applied when it is given another argument \lstinline{n}.
From this point of view, \lstinline{Add} is a function of type \lstinline{Nat → (Nat → Nat)} which is the same as
\lstinline{Nat → Nat → Nat} because in Lean the arrow \lstinline{→} is right associative.

In a certain sense, currying makes functions conceptually simpler;
all functions are single variable, it's just that some return other functions.

\paragraph{Integers and lists}
Functions belonging to $\RPP$ have $\Z^n$ as their domain and codomain,
so we need a way to represent and work with integer tuples.

The good news is that integers are already defined in Lean.
Here is their definition:
\begin{lstlisting}
inductive int : Type
| of_nat (n : ℕ) : int
| neg_succ_of_nat (n : ℕ) : int  
\end{lstlisting}
The value \lstinline{of_nat n} represents the natural number \lstinline{n : ℕ} as an integer,
while \lstinline{neg_succ_of_nat n} represents the negative number \lstinline{-(n+1)}.
Of course it's not the definition we've just seen that gives this meaning to the \lstinline{int}s;
rather, it's the functions defined on them (like addition, subtraction etc.).

Immediately after the definition, some notation is introduced:
\begin{itemize}
\item \lstinline{ℤ} stands for \lstinline{int}
\item in a context in which an integer is expected, if instead a natural number is supplied,
the function \lstinline{of_nat} will be automatically applied on the natural number.
This convenient feature is called coercion.
\item for every \lstinline{n : ℕ}, \lstinline{-[1+ n]} stands for \lstinline{neg_succ_of_nat n}.
This notation is almost never used.
\end{itemize}
\begin{lstlisting}
notation `ℤ` := int

instance : has_coe nat int := ⟨int.of_nat⟩

notation `-[1+ ` n `]` := int.neg_succ_of_nat n
\end{lstlisting}
As an example of a function from \lstinline{ℤ} to \lstinline{ℤ}, this is negation:
\begin{lstlisting}
def neg : ℤ → ℤ
| (of_nat n) := neg_of_nat n
| -[1+ n]    := succ n
\end{lstlisting}

We're interested not just in integers, but in tuples of integers.
We can implement the concept of a tuple in many ways,
but a particularly simple one is through the use of lists,
a very common data structure in computer science.

Let's consider lists of natural numbers, i.e. the type \lstinline{list ℕ}.
This is a list of 5 elements:
\begin{lstlisting}
open list
#reduce [4, 5, 7, 2, 5] -- [4, 5, 7, 2, 5]
\end{lstlisting}
The first element of a list is the \textit{head}
\begin{lstlisting}
#reduce head [4, 5, 7, 2, 5] -- 4
\end{lstlisting}
while the other elements are the \textit{tail}.
\begin{lstlisting}
#reduce tail [4, 5, 7, 2, 5] -- [5, 7, 2, 5]
\end{lstlisting}
Given \lstinline{n : ℕ} and \lstinline{l : list ℕ} we can obtain a new list
\lstinline{cons n l} (also written as \lstinline{n :: l}) such that
\lstinline{head (n :: l) = n} and \lstinline{head (n :: l) = l}
\begin{lstlisting}
#reduce cons 2 [4, 5, 7, 2, 5] -- [2, 4, 5, 7, 2, 5]
#reduce 2 :: [4, 5, 7, 2, 5]   -- alternative notation  
\end{lstlisting}
and ultimately, every list can be obtained by starting with \lstinline{nil},
the empty list, and repeatedly using \lstinline{cons}.
\begin{lstlisting}
#reduce nil -- the empty list
#reduce []  -- alternative notation 
#reduce cons 4 (cons 5 (cons 7 (cons 2 (cons 5 nil))))
            -- [4, 5, 7, 2, 5]
#reduce 4 :: 5 :: 7 :: 2 :: 5 :: []
            -- alternative notation
\end{lstlisting}
This might suggest a definition of lists of naturals:
a \lstinline{list_nat} is either the empty list \lstinline{nil_nat},
or \lstinline{cons_nat hd tl} where \lstinline{hd : ℕ} and \lstinline{tl : list_nat}
are respectively the head and tail of the list:
\begin{lstlisting}
inductive list_nat : Type
| nil_nat : list_nat
| cons_nat (hd : ℕ) (tl : list_nat) : list_nat
\end{lstlisting}
There's nothing special about using natural numbers.
We can use the same procedure to define lists of integers:
\begin{lstlisting}
inductive list_int : Type
| nil_int : list_int
| cons_int (hd : ℤ) (tl : list_int) : list_int
\end{lstlisting}
but having to define different types of lists for each type of element is pretty cumbersome.
Instead, we can define lists for a generic type \lstinline{T} using \textit{dependent types}:\footnote{a Lean user would probably frown at this,
because it would be best to choose an explicit universe \lstinline{u} and work with \lstinline{Type u}.}
\begin{lstlisting}
inductive list (T : Type) : Type
| nil : list
| cons (hd : T) (tl : list) : list
\end{lstlisting}
Rather than having \lstinline{list_nat}, \lstinline{list_int}...
we use \lstinline{list ℕ}, \lstinline{list ℤ}...
\lstinline{list α} where \lstinline{α} is any type.

We can see how useful dependent types are by defining the function \lstinline{length}
which returns the number of elements of a list:
\begin{lstlisting}
def length {α : Type*} : list α → ℕ
| []       := 0
| (a :: l) := length l + 1
\end{lstlisting}
Note that we can use \lstinline|{α : Type*}| to refer to a generic type \lstinline{α}.
If instead we had stuck to \lstinline{list_nat}, \lstinline{list_int}...
now we would have to define \lstinline{length_nat}, \lstinline{length_int}... separately for each type.

We will identity tuples of $n$ elements $\Z^n$ with lists in \lstinline{list ℤ} of length \lstinline{n}.

\section{The definition in Lean}

\paragraph{Syntax and semantics}
Let's now ask ourselves: how can we define in a satisfactory way the class of functions $\RPP$ in Lean,
using just types and functions?
We'd like to be able to do proofs by induction over $\RPP$, like in proposition \ref{rppinv}, so we'll need to define an inductive type.

The key is thinking about $\RPP$ not as a class of functions, but as a small programming language.
In this sense, we can write down "programs" like our square function
\[(\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw)\]
but we should not view it only as a function belonging to $\Z^3 \to \Z^3$,
but also as the sentence
"$(\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw)$"
which can then be interpreted as the mathematical function belonging to $\Z^3 \to \Z^3$.

We thus separate between the \textit{syntax} and the \textit{semantics} of our language.
\begin{itemize}
\item The syntax are the rules which governs how to assemble well-structured sentences.
For example, the selection symbol $\rppIf$ should be followed by three other $\RPP$ functions;
if we write $\rppIf[\rppSu, \rppPr] \rppCo \rppNe$ we get a non-valid sentence.
\item The semantics is the meaning we give to (well-structured) sentences -
in our case, they are intepreted as functions $\Z^n \to \Z^n$.
\end{itemize}

A possible way to define $\RPP$ functions in Lean is
\begin{itemize}
\item define the type $\RPP$ which has for elements syntactically-correct sentences of $\RPP$
\item define a function $\mathsf{evaluate} : \RPP \to (\Z^n \to \Z^n)$ which assigns to each $\RPP$-sentence its intended meaning,
namely a function $\Z^n \to \Z^n$.
\end{itemize}
Note that this is not the only way in which this task can be accomplished;
we will discuss other methods at the end of this chapter.

We thus define the type $\RPP$ as follows:
\begin{lstlisting}
inductive RPP : Type
| Id (n : ℕ) : RPP
| Ne : RPP
| Su : RPP
| Pr : RPP
| Sw : RPP
| Co (f g : RPP) : RPP
| Pa (f g : RPP) : RPP
| It (f : RPP) : RPP
| If (f g h : RPP) : RPP
\end{lstlisting}
and also introduce custom notation:
\begin{lstlisting}
-- the numbers 50 and 55 denote the precedence -
-- simply put, Ne ;; Su ‖ Pr is intepreted as
-- Ne ;; (Su ‖ Pr), not (Ne ;; Su) ‖ Pr
infix `;;` : 50 := Co
infix `‖` : 55 := Pa
\end{lstlisting}
so it's now possible to write expressions like
\begin{lstlisting}
#check It Su ;; (Id 1 ‖ If Sw Pr Su) -- RPP
\end{lstlisting}
Remember that by remark \ref{different_arity}, it makes sense to consider the series composition of functions of different arity,
as long as we give them the meaning specified in the remark.

Talking about arity, how do we deal with it?
In order to define $\mathsf{evaluate}$ and give meaning to $\RPP$,
we must be able to define a concept of arity,
otherwise we'll have trouble with parallel composition of two functions \lstinline{f ‖ g} -
the arity of \lstinline{f} must be known,
otherwise it's impossible to tell what to apply \lstinline{g} to.

Luckily, we can reconstruct the arity of an \lstinline{RPP} just by looking at its symbolic representation:
\begin{lstlisting}
def arity : RPP → ℕ
| (Id n)     := n
| Ne         := 1
| Su         := 1
| Pr         := 1
| Sw         := 2
| (f ;; g)   := max f.arity g.arity
| (f ‖ g)    := f.arity + g.arity
| (It f)     := f.arity + 1
| (If f g h) := max (max f.arity g.arity) h.arity + 1
\end{lstlisting}
Note that \lstinline{f.arity} is the same as \lstinline{(arity f)}.
This is a recursive function:
there are 5 base cases and in the other 4 the value of \lstinline{arity} is reconstructed from smaller sub-terms.

It's now possible to define some \lstinline{RPP}-sentences in Lean
\begin{lstlisting}
def inc := It Su
def dec := It Pr
def mul := It inc
def square := Id 1 ‖ Sw ;; inc ;; mul ;; dec ;; Id 1 ‖ Sw
\end{lstlisting}
and it's even possible to calculate their arity
\begin{lstlisting}
#reduce square.arity -- outputs "3"
\end{lstlisting}
but we haven't yet given their meaning as functions.

\paragraph{The \lstinline{evaluate} function}
We are now ready to define \lstinline{evaluate} (\lstinline{ev} for short).
The function $\mathsf{ev}$ should take $\RPP$-sentences and return functions $\Z^n \to \Z^n$,
so in Lean we will define it as a function of type
\begin{lstlisting}
RPP → (list ℤ → list ℤ)
\end{lstlisting}
which in Lean is the same as
\begin{lstlisting}
RPP → list ℤ → list ℤ.
\end{lstlisting}
Here's how we do it:
\begin{lstlisting}
def ev : RPP → list ℤ → list ℤ
| (Id n)     l                    := l
| Ne         (x :: l)             := -x :: l
| Su         (x :: l)             := (x + 1) :: l
| Pr         (x :: l)             := (x - 1) :: l
| Sw         (x :: y :: l)        := y :: x :: l
| (f ;; g)   l                    := ev g (ev f l)
| (f ‖ g)    l                    := ev f (take f.arity l) ++
                                     ev g (drop f.arity l)
| (It f)     (x :: l)             := x :: ((ev f)^[↓x] l)
| (If f g h) (0 :: l)             := 0 :: ev g l
| (If f g h) (((n : ℕ) + 1) :: l) := (n + 1) :: ev f l
| (If f g h) (-[1+ n] :: l)       := -[1+ n] :: ev h l
| _          l                    := l

notation `‹` f `›` := ev f
\end{lstlisting}
We will write \lstinline{‹f›} to mean the function of type \lstinline{list ℤ → list ℤ} given by \lstinline{ev f}.

Here's a case-by-case analysis:
\begin{itemize}
\item \lstinline{‹Id n› l} is the original list \lstinline{l}, unchanged.
\item \lstinline{‹Ne› (x :: l)} reduces to \lstinline{-x :: l}, which is same list but with the head of opposite sign.
\item \lstinline{‹Su› (x :: l)} reduces to the same list but with the head incremented by one.
\item \lstinline{‹Pr› (x :: l)} reduces to the same list but with the head decremented by one.
\item \lstinline{‹Sw› (x :: y :: l)} reduces to the same list but with the first two elements swapped.
\item \lstinline{‹f ;; g› l} successively applies \lstinline{‹f›} and \lstinline{‹g›} to the list.
\item \lstinline{‹f ‖ g› l} applies \lstinline{‹f›} to the first \lstinline{f.arity} elements of the list,
applies \lstinline{‹g›} to the remaining elements of the list,
and then joins the two parts through \lstinline{append} (which is the \lstinline{(++)} operator).
\item \lstinline{‹It f› (x :: l)} leaves the head unchanged and applies \lstinline{‹f›} to the tail \lstinline{↓x} times,
where \lstinline{↓x} is defined as in definition \ref{rppdef}.
\item \lstinline{‹If f g h› (0 :: l)} leaves the head unchanged and applies \lstinline{‹g›} to the tail.
\item \lstinline{‹If f g h› (((n : ℕ) + 1) :: l)} is the case where the head is a positive number
(a natural number plus \lstinline{1}),
and as such leaves the head unchanged and applies \lstinline{‹f›} to the tail.
\item \lstinline{‹If f g h› (-[1+ n] :: l)} is the case where the head is a negative number,
and as such leaves the head unchanged and applies \lstinline{‹h›} to the tail.
\item In all cases not considered (for example, applying \lstinline{‹Ne›} to an empty list) the whole list remains unchanged. 
\end{itemize}

The reader is invited to compare this definition with the one given in definition \ref{rppdef}.

Let's see some examples:
\begin{lstlisting}
#check ‹It Su ;; (Id 1 ‖ If Sw Pr Su)› -- list ℤ → list ℤ
-- #eval is similar to #reduce
-- but in this case gives more readable output
#eval ‹inc› [3, 4] -- [3, 7]
#eval ‹square› [19, 0, 0] -- [19, 361, 0]
\end{lstlisting}
It magically works. We finally have our definition formalized in Lean.

It is worth noting that even though lists supplied to \lstinline{‹f›} are supposed to have length equal to \lstinline{f.arity},
this is never enforced.
So we are free to apply \lstinline{‹f›} to a list which is too short or too long.
If it's too short, unspecified things will happen, we don't care.
If it's too long, only the first \lstinline{f.arity} items are utilized and affected,
and this is guaranteed by theorem \lstinline{ev_split} which we will prove in Lean.
So, when we apply \lstinline{RPP} functions to a list, we'll have to make sure that
\lstinline{f.arity ≤ l.length}.

\paragraph{The \lstinline{inverse} function}
It's not hard to convert our proposition \ref{rppinv} into a function definition:
\lstinline{inv : RPP → RPP} which given \lstinline{f : RPP} returns its inverse.
\begin{lstlisting}
def inv : RPP → RPP
| (Id n)     := Id n
| Ne         := Ne
| Su         := Pr
| Pr         := Su
| Sw         := Sw
| (f ;; g)   := inv g ;; inv f
| (f ‖ g)    := inv f ‖ inv g
| (It f)     := It (inv f)
| (If f g h) := If (inv f) (inv g) (inv h)

notation f `⁻¹` := inv f
\end{lstlisting}
Now it's possible to define \lstinline{dec} simply as \lstinline{inc⁻¹}.

We will also prove in Lean that \lstinline{f⁻¹} really is the inverse (in the functional sense) of \lstinline{f},
but it will require some work.
\section{Differences with the original definition}
The definition of \lstinline{RPP} functions we've given differs quite a bit from the original one.
Every change has been made in the name of simplicity:
theorem proving in Lean is hard enough,
we don't need to make it harder by choosing inconvenient definitions.
Below is a list of changes, not only for completeness' sake but also to illustrate the kind of reasoning which goes on
when formalizing definitions in Lean.
\begin{itemize}
\item In the original definition,
in the iterator $\rppIt$ and selection $\rppIf$ the last element of the tuple is checked,
not the first one (the head).
It was more convenient to work with the first element because of the definition of lists:
it's much easier to consider a list's head and tail than its last element and the elements before the last.
\item We have defined $\rppId_n$ as a $n$-ary function, while originally it was just unary.
Having a $n$-ary identity function is very useful,
because we can use parallel composition as in remark \ref{different_arity},
and also because we have the possibility of having a $0$-ary function, which is not useless in some cases.
\item The original $\RPP$ functions are defined as the union $\cup_{n \in \N}\RPP^n$
where $\RPP^n$ are the $n$-ary $\RPP$ functions.
A similar decision could've been made in Lean by definining \lstinline{RPP n} as a dependent type
with parameter \lstinline{n : ℕ},
but it turned out that it was possible to calculate the arity of an \lstinline{RPP} simply by looking at
the corresponding \lstinline{RPP}-sentence, which is what we did when we defined the function \lstinline{arity}.
This rendered superfluous using dependent types and separating \lstinline{RPP} based on their arity.

There's a reason we tried to avoid dependent types wherever possible
(which also led to the use of \lstinline{list}s instead of \lstinline{vector}s):
at least in Coq (which is another proof assistants we used at the beginning of the project)
working with dependent types is often painful, because Coq doesn't recognize that certain types are the same.
For example, elements of \lstinline{RPP (n + 1)} and \lstinline{RPP (1 + n)} cannot be compared even though it is (demonstrably!) true that
\lstinline{n + 1 = 1 + n}. To get around this, it's possible to use something called John Major's Equality
to state the equality of two objects with seemingly different types,
but this involves the invocation of an additional axiom and is in general annoying to use.
Other ways to deal with the problem exist,
but our choice ended up being avoiding dependent types completely.
As someone on the internet says,
\begin{displayquote}
Coq has this really powerful type system, but... don't use it.
\end{displayquote}
By extension, we also avoided them in Lean.
\item When defining the iterator $\rppIt[f] (x, x_1, \dots, x_n)$ it's not immediately clear what to do when $x < 0$.
In our definition, nothing happens, as $f$ in general is applied $\downarrow x = 0$ times.
In the original definition, $f$ is instead applied $| x |$ times - let's call this iterator $\rppIta$.

Reversibility gifts us with a third option: if $x < 0$,
we can apply $f$ a negative amount of times - or in other words, we can apply $f^{-1}$ for $- x$ times.
Let's call this iterator $\rppItr$.
Its usage leads to more natural definitions:
for example, our function $\rppinc (n, x) = (\rppIt[\rppSu]) (n, x)$ returns $(n, x+n)$ only if $n \ge 0$.
If instead we use $\rppItr$, suddenly $(\rppItr[\rppSu]) (n, x) = (n, x + n)$ for all values of $n \in \Z$.

So why didn't we use $\rppItr$?
Because our $\rppIt$ is the most versatile option:
we can define both $\rppIta$ and $\rppItr$ in terms of $\rppIt$,
by using the fact that $\rppIt$ doesn't do anything when the first argument is negative:
\begin{align*}
  \rppIta[f] = \rppIt[f] \rppCo \rppNe \rppCo \rppIt[f] \rppCo \rppNe \\
  \rppItr[f] = \rppIt[f] \rppCo \rppNe \rppCo \rppIt[f^{-1}] \rppCo \rppNe
\end{align*}
For example, in the case of $\rppIta[f](x,x_1,\cdots,x_n)$, if $x \ge 0$ then the first $\rppIt$ applies $f$ for $x$ times,
then $x$ changes sign and becomes $-x$ with $\rppNe$, then the second $\rppIt$ doesn't do anything because $- x < 0$,
and finally $-x$ changes sign again to $x$; if instead $x < 0$, only the second $\rppIt$ does something.

Another reason to prefer $\rppIt$ over $\rppItr$ is that in the definition of \lstinline{evaluate},
using $\rppItr$ it's hard to convince Lean (or Coq) that the function terminates (that is, it doesn't run on an infinite loop).
Since every function in Lean must terminate (otherwise there would be consistency issues), Lean rejects the definition.
There are ways to get around this - but once again we follow the path of least resistance and just get on with $\rppIt$.
\end{itemize}

After seeing all these changes you might ask yourself - is this still the original $\RPP$?
What's the point of formalizing a definition in Lean if in the process we change the definition completely?

We think that yes, we can still identify what we've constructed as the original functions,
because in a way, the \textit{essence} of what $\RPP$ is has not been altered:
a class of functions which is reversible by construction and that is $\PRF$-complete.
We shouldn't view definitions as something unchanging and rigid, especially in rapidly evolving fields.
Definitions should be molded and modified to fit our needs,
because that's why we created them in the first place.


\chapter{Theorem proving}

\section{Some examples of Lean proofs}

\paragraph{Reflexivity}

We define the type \lstinline{Nat} similarly as before
\begin{lstlisting}
inductive Nat : Type
| O : Nat -- it's a capital o, not a zero
| S (n : Nat) : Nat

open Nat
#reduce O -- represents 0
#reduce S O -- represents 1
#reduce O.S -- also represents 1
            -- we'll use this notation
#reduce O.S.S -- represents 2

\end{lstlisting}
together with addition
\begin{lstlisting}
def Add : Nat → Nat → Nat
| m O      := m
| m (S n') := (Add m n').S  

-- if n m : Nat then n + m is defined as Add n m
infix `+` := Add
\end{lstlisting}
  
Now let's prove some theorems about these objects.
Let's start with something simple: we want to prove,
beyond the shadow of a doubt, that \lstinline{O = O}.
We open our code editor and type this:
\begin{lstlisting}
lemma O_eq_O : O = O :=
begin

end
\end{lstlisting}
If we now place the cursor between \lstinline{begin} and \lstinline{end}
this appears in the sidebar:
\begin{lstlisting}
  1 goal
  
  ⊢ O = O
\end{lstlisting}
This is called the \textbf{tactic state}.
The line beginning with the turnstile \lstinline{⊢ O = O} is our \textbf{goal}.
We can write commands called \textbf{tactics} which helps us solve goals.
In this case, the goal is an equality in which the left-hand side happens to coincide perfectly with the right-hand side,
so we can solve our goal using the \lstinline{refl} command
\begin{lstlisting}
lemma O_eq_O : O = O :=
begin
  refl,
end
\end{lstlisting}
and placing the cursor just after the comma, a pleasant message appears:
\begin{lstlisting}
  goals accomplished
\end{lstlisting}
The name \lstinline{refl} stands for "reflexivity",
which is a property of equality (for any \lstinline{a}, it is true that \lstinline{a = a}).
Let's try something more involved: $2 + 2 = 4$.
\begin{lstlisting}
lemma two_plus_two : O.S.S + O.S.S = O.S.S.S.S :=
begin

end
\end{lstlisting}
This time the left-hand side and the right-hand side do not look identical.
However, there's something interesting to note:
\begin{lstlisting}
#reduce O.S.S.S.S -- outputs "O.S.S.S.S"
#reduce O.S.S + O.S.S -- also outputs "O.S.S.S.S"
\end{lstlisting}
that is, \lstinline{O.S.S + O.S.S} reduces to \lstinline{O.S.S.S.S}.
Since the left-hand side and the right-hand side reduce to the same element,
they are \textbf{definitionally equivalent} and so we can use the tactic \lstinline{refl} again:
\begin{lstlisting}
lemma two_plus_two : O.S.S + O.S.S = O.S.S.S.S :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}
Remember that by definition of \lstinline{Add}, for any \lstinline{n m : Nat},
we have that \lstinline{n + m.S = (n + m).S}.
We can express this with another theorem
\begin{lstlisting}
lemma plus_S (n m : Nat) : n + m.S = (n + m).S :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}
which again can be solved using \lstinline{refl}, because \lstinline{n + m.S} reduces to \lstinline{(n + m).S}.
By the way, we could write \lstinline{theorem} instead of \lstinline{lemma}: the difference is only stylistic.
Similarly, by definition of \lstinline{Add}, \lstinline{n + O = n} for all \lstinline{n : Nat}
\begin{lstlisting}
lemma plus_O (n : Nat) : n + O = n :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}

\paragraph{Induction and rewrite}

If we instead we try to prove in the same way that \lstinline{O + n = n},
\begin{lstlisting}
def O_plus (n : Nat) : O + n = n :=
begin
  refl,
end
\end{lstlisting}
something surprising happens:
\begin{lstlisting}
invalid apply tactic, failed to unify
    O+n = n
  with
    ?m_2 = ?m_2
  state:
  n : Nat
  ⊢ O+n = n
\end{lstlisting}
our trusted \lstinline{refl} has, alas, failed.
This is because \lstinline{O + n} is \textbf{not} definitionally equivalent to \lstinline{n}:
the function \lstinline{Add} defines two definitional equivalences
(\lstinline{m + O = m} and \lstinline{m + n.S = (m + n).S})
and there's nothing regarding \lstinline{0 + n} when we have a generic \lstinline{n : Nat}.
However, two things can still be equal even if they are not definitionally equivalent.

To prove \lstinline{O_plus} we need something stronger: the tactic \lstinline{induction}.
Let's try it:
\begin{lstlisting}
def O_plus (n : Nat) : O + n = n :=
begin
  induction n using n hn,
end
\end{lstlisting}
Now the tactic state has become
\begin{lstlisting}
  2 goals

  case Nat.O
  ⊢ O+O = O

  case Nat.S
  n: Nat
  hn: O+n = n
  ⊢ O+n.S = n.S
\end{lstlisting}
What happened is that we used induction:
to prove a property \lstinline{P n} (in this case, \lstinline{O + n = n}) for all \lstinline{n : Nat},
it suffices to prove that \lstinline{P O} holds and that from \lstinline{P n} follows \lstinline{P n.S}.
The first subgoal is the base case \lstinline{O+O = O}, and can be solved using \lstinline{refl}
\begin{lstlisting}
lemma O_plus (n : Nat) : O + n = n :=
begin
  induction n with n hn,
  refl,
end
\end{lstlisting}
Only the second goal remains
\begin{lstlisting}
  1 goal

  case Nat.S
  n : Nat
  hn : O+n = n
  ⊢ O+n.S = n.S
\end{lstlisting}
This means that \lstinline{n} is an element of \lstinline{Nat}
and that we have an hypothesis named \lstinline{hn} which tells us that \lstinline{O+n = n}.
Our goal is to prove that \lstinline{O+n.S = n.S}.
A proof of this fact would go somewhat like this:
\begin{enumerate}
\item by lemma \lstinline{plus_S} we have \lstinline{O+n.S = (O+n).S}
\item by the induction hypothesis \lstinline{hn} we have \lstinline{O+n = n} and by substitution we get \lstinline{(0+n).S = n.S}
\end{enumerate}
so \lstinline{O+n.S = (O+n).S = n.S}, and this completes the proof.

We can capture this act of substituting a term in an equation with an equivalent one using the tactic \lstinline{rw} ("rewrite"):
for example, \lstinline{rw plus_S} search in the goal for subterms of the form \lstinline{O+n.S} and substitutes them with \lstinline{(O+n).S}.
More generally, given an equality \lstinline{h : a = b}, calling \lstinline{rw h} substitutes occurrences of \lstinline{a} in the goal with \lstinline{b}.
\begin{lstlisting}
lemma O_plus (n : Nat) : O + n = n :=
begin
  induction n with n hn,
  refl,
             -- goal: O+n.S = n.S
  rw plus_S, -- goal: (O+n).S = n.S
  rw hn,     -- goal: n.S = n.S
             -- refl is automatically called with rw,
             -- so: goal accomplished!
end
\end{lstlisting}
Let's tackle one more theorem: the commutativiy of addition
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin

end
\end{lstlisting}
Again, using \lstinline{refl} doesn't work, so we use induction.
We have a choice: using induction on \lstinline{n} or \lstinline{m};
note that doing induction on one or the other is not the same,
because \lstinline{n} and \lstinline{m} have asymmetric roles in the definition of \lstinline{Add}.
In particular, the second argument gets "broken down" at each step
(since \lstinline{n + m.S = (n + m).S}) while the first argument doesn't change.
Thus, in this case the best choice is induction on \lstinline{m}.
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin
  induction m with m hm, -- 2 goals

                         case Nat.O
                         n: Nat
                         ⊢ n+O = O+n

                         case Nat.S
                         nm: Nat
                         hm: n+m = m+n
                         ⊢ n+m.S = m.S+n
end
\end{lstlisting}
We can deal with the base case \lstinline{n+O = O+n} by using \lstinline{rw} with our two lemmas
\lstinline{plus_O : n + O = n} and \lstinline{O_plus : O + n = n}:
\begin{lstlisting}
  rw plus_O, rw O_plus, -- first goal vanquished
\end{lstlisting}
So now we have hypothesis \lstinline{hm : n+m = m+n} and goal \lstinline{n+m.S = m.S+n}.
We can use \lstinline{rw plus_S} to change the goal to \lstinline{(n+m).S = m.S+n},
and if we could further rewrite it to \lstinline{(n+m).S = (m+n).S} then we would use the hypothesis
to solve the goal.
Problem is, we don't have a theorem which states that \lstinline{m.S+n = (m+n).S} - but we leave it as an exercise for the reader.
Having called it \lstinline{S_plus} we can thus conclude our proof
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin
  induction m with m hm,
  rw plus_O, rw O_plus,
  rw plus_S, rw S_plus, rw hm, -- goal accomplished
end
\end{lstlisting}
So, we've now learned some basics about theorem proving in Lean,
but we don't know anything yet about what proofs \textit{are} and how they fit in the general scheme of things.
There is a lot to be learned.

\section{Curry-Howard correspondence}

The following section is not necessary to understand the rest of the thesis,
so the busy reader can skip it.

In Lean, things like propositions and proofs are not completely separated from data objects like types and elements of types.
We previously stated that in Lean, everything has a type, and we can see what type a certain object has by using the \lstinline{#check} command.

So, let's feed random stuff to \lstinline{#check}.
\begin{lstlisting}
#check O -- Nat
#check Nat -- Type
#check Type -- Type 1
#check Type 1 -- Type 2
              -- it's an infinite family of types
              -- for each u, Type u is an element of Type (u+1)
#check Add -- Nat → Nat → Nat
#check Nat → Nat → Nat -- Type

-- some theorem names...
#check two_plus_two -- O.S.S+O.S.S = O.S.S.S.S
#check O_plus -- ∀ (n : Nat), O+n = n
#check plus_comm -- ∀ (n m : Nat), n+m = m+n

-- ..and their statements
#check O.S.S+O.S.S = O.S.S.S.S -- Prop
#check ∀ (n : Nat), O+n = n -- Prop
#check ∀ (n m : Nat), n+m = m+n -- Prop

#check Prop -- Type
\end{lstlisting}
So, there is a type called \lstinline{Prop} and its elements are propositions.
Any proposition, like \lstinline{∀ (n : Nat), O+n = n}, is in turn a type - but what exactly are its elements?

The elements of a proposition are proofs of that proposition.
This means that what we have called \lstinline{two_plus_two} is a proof of the fact that \lstinline{O.S.S+O.S.S = O.S.S.S.S}.
What we mean by proving a proposition, is finding an element whose type is that proposition.
For example, here is the definition of the proposition \lstinline{true}:
\begin{lstlisting}
inductive true : Prop -- this is a Prop, not a Type
| intro : true    
\end{lstlisting}
How do we know that \lstinline{true} is true? Because it has an element, \lstinline{true.intro}:
\begin{lstlisting}
lemma true_is_true : true :=
begin
  exact true.intro, -- if we have an explicit element
                    -- of the proposition to be proven,
                    -- we can use the tactic exact
end
\end{lstlisting}
The proposition \lstinline{false} is defined like this:
\begin{lstlisting}
inductive false : Prop  
\end{lstlisting}
It's a type with no elements, so \lstinline{false} can't be proven.

Suppose that $A$ is a proposition in classical logic.
Then it is true that $A \Rightarrow A$ (we can derive it using natural deduction, for example).
In Lean this fact can be expressed as the proposition \lstinline{A → A}, and it's something provable:
\begin{lstlisting}
lemma A_implies_A (A : Prop) : A → A :=
begin
  -- 1 goal
  -- A: Prop
  -- ⊢ A → A
end
\end{lstlisting}
At the left of the implication arrow we have \lstinline{A}:
we can thus turn \lstinline{A} into an hypothesis using the \lstinline{intro} tactic
\begin{lstlisting}
lemma A_implies_A (A : Prop) : A → A :=
begin
  intro h, -- creates a hypothesis h : A
  -- 1 goal
  -- A : Prop
  -- h : A
  -- ⊢ A
  exact h, -- goal accomplished
end
\end{lstlisting}
Let's forget for a moment that \lstinline{A} is a proposition, let's think of it as just a type.
Then something funny happens: we see that \lstinline{A → A} is just the type of functions from \lstinline{A} to \lstinline{A}.
If we can exhibit an element of this type, then we have proven that \lstinline{A → A}.
But this is easy enough: we can use the identity function
\begin{lstlisting}
def A_implies_A' (A : Prop) : A → A
| h := h    
\end{lstlisting}
and this proves the proposition, but also defines a function.
It can be interpreted like this:
if we have a proof \lstinline{h : A} and we have to prove \lstinline{A} then we can just exhibit \lstinline{h}; that's it.

Amazingly, it turns out all proofs are really just functions: we can see this using the \lstinline{#print} command,
which given a function prints out its definition.
\begin{lstlisting}
#print Add
  -- def Add : Nat → Nat → Nat :=
  -- λ (n m : Nat), Nat.rec n (λ (m' n_m' : Nat), n_m'.S) m
#print A_implies_A
  -- theorem A_implies_A : ∀ (A : Prop), A → A :=
  -- λ (A : Prop) (h : A), h
#print O_plus
  -- theorem O_plus : ∀ (n : Nat), O+n = n :=
  -- λ (n : Nat),
  --   Nat.rec (eq.refl (O+O))
  --     (λ (n : Nat) (hn : O+n = n),
  --        (id (eq.rec (eq.refl (O+n.S = n.S))
  --                    (plus_S O n))).mpr
  --          ((id (eq.rec (eq.refl ((O+n).S = n.S)) hn)).mpr
  --            (eq.refl n.S)))
  --     n
\end{lstlisting}
When we proved \lstinline{O_plus} we didn't explicitly write a function,
we used tactics, but it's important to notice that the sequence of tactics \textit{isn't} the proof -
instead, tactics generate proofs (functions).
To further illustrate this point, notice that we can also define the function \lstinline{Add} using tactics:
\begin{lstlisting}
def Add_tactic (n m : Nat) : Nat :=
begin
  -- 1 goal
  -- n m : Nat
  -- ⊢ Nat
  -- to "solve" the goal we have to provide a Nat
  induction m with m' n_m', -- induction on m
  exact n, -- base case, m=O: we return n
  exact n_m'.S, -- inductive case, Add n m' = n_m'
                -- we have to provide Add n m'.S:
                -- we return n_m'.S
                -- "goal accomplished"
end

#reduce Add_tactic O.S.S O.S.S.S.S -- O.S.S.S.S.S.S
\end{lstlisting}
And notice! \lstinline{Add} is a recursive function, so to define it we had to use induction.
Going back to \lstinline{#print Add} and \lstinline{#print O_plus},
we can see that both call a function called \lstinline{Nat.rec}. Let's investigate:
\begin{lstlisting}
#check Nat.rec
  -- Nat.rec : ?M_1 O →
  --           (Π (n : Nat), ?M_1 n → ?M_1 n.S) →
  --           Π (n : Nat), ?M_1 n
\end{lstlisting}
If we intepret \lstinline{?M_1} as a proposition \lstinline{P : Nat → Prop} which ranges over a \lstinline{Nat},
then \lstinline{Nat.rec} reads off as
"if \lstinline{P O} holds true and for all \lstinline{n : Nat},
\lstinline{P n} implies \lstinline{P n.S}, then \lstinline{P n} holds for all \lstinline{n : Nat}" which is the principle of induction.
If instead we interpret \lstinline{?M_1} as a function \lstinline{f : Nat → T} with domain \lstinline{Nat} and codomain a certain type \lstinline{T},
then \lstinline{Nat.rec} reads off as
"if we define \lstinline{f O} and for all \lstinline{n : Nat},
given \lstinline{f n} we define \lstinline{f n.S}, then we have defined a function \lstinline{Nat → T}" which is how we define recursive functions.

\lstinline{Nat.rec} is called the induction principle of \lstinline{Nat},
and is auto-generated as soon as \lstinline{Nat} is defined.
Every type generates an induction principle - even simple types like our earlier \lstinline{weekday} -
hence every type is defined using the \lstinline{inductive} keyword.
The same induction principle is used both for recursive functions and inductive proofs.

The remarkable thing about Lean is that the concepts of types and propositions, functions and proofs are united in a single mechanism,
which results in a particularly simple foundation for mathematics and computing.
This concept is called the \textbf{Curry-Howard correspondence} and it's not something unique to Lean -
it's a common characteristic of many theorem prover, especially those based on (intuitionistic) type theory.

\end{document}