% base
\documentclass{book}

% characters
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}

\usepackage{comment}

% math
\usepackage{amsmath}
\usepackage{mathtools}

% graphics
\usepackage{mathdots}
\usepackage{nicematrix}
\setcounter{MaxMatrixCols}{20}
\usepackage{rotating}
\definecolor{lblue}{rgb}{.80, .90, .95}
\definecolor{lorange}{rgb}{1, .85, .60}

\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1} % red
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4} % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}  % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}    % green

\usepackage{graphicx}

% font
% \usepackage{mathpazo}
% \usepackage{eulervm}
\usepackage{amsfonts}

% code
\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean}
\usepackage{float}

% sections
\usepackage{emptypage}
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bfseries}{\thechapter.}{20pt}{\huge}

% theorems
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\theoremstyle{plain}
\newtheorem{proposition}{Proposition}

% hyphenation
\usepackage{hyphenat}
\hyphenation{pri-mi-tive}

% other
\usepackage[colorlinks=true]{hyperref}
\usepackage{csquotes}

% macro
% \newcommand{\svdots}{\scriptscriptstyle \boldsymbol \vdots}
\newcommand{\perm}[1]{\scriptstyle \left\rmoustache #1 \right\rmoustache}
\newcommand{\bloch}[2]{\Block[draw=white,fill=lblue,line-width=.5mm,rounded-corners]{#1}{#2}} % https://en.wikipedia.org/wiki/Ernest_Bloch
\newcommand{\oloch}[2]{\Block[draw=white,fill=lorange,line-width=.5mm,rounded-corners]{#1}{#2}}
\newcommand{\conv}[1]{\overbracket[.5pt][1pt]{\underbracket[.5pt][1pt]{#1}}}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\RPP}{\mathsf{RPP}}
\newcommand{\ORPP}{\mathsf{ORPP}}
\newcommand{\rppf}{\mathsf{f}}
\newcommand{\rppg}{\mathsf{g}}
\newcommand{\rpph}{\mathsf{h}}
\newcommand{\rppId}{\mathsf{Id}}
\newcommand{\rppNe}{\mathsf{Ne}}
\newcommand{\rppSu}{\mathsf{Su}}
\newcommand{\rppPr}{\mathsf{Pr}}
\newcommand{\rppSw}{\mathsf{Sw}}
\newcommand{\rppCo}{\fatsemi}
\newcommand{\rppPa}{\Vert}
\newcommand{\rppIt}{\mathsf{It}}
\newcommand{\rppIta}{\mathsf{Ita}}
\newcommand{\rppItr}{\mathsf{Itr}}
\newcommand{\rppIf}{\mathsf{If}}
\newcommand{\rppinc}{\mathsf{inc}}
\newcommand{\rppdec}{\mathsf{dec}}
\newcommand{\rppmul}{\mathsf{mul}}
\newcommand{\rppsquare}{\mathsf{square}}
\newcommand{\rppcp}{\mathsf{cp}}
\newcommand{\rpptr}{\mathsf{tr}}
\newcommand{\rppcu}{\mathsf{cu}}
\newcommand{\rppcustep}{\mathsf{cu_{step}}}
\newcommand{\rppdiv}{\mathsf{div}}
\newcommand{\rppdivstep}{\mathsf{div_{step}}}
\newcommand{\rppsqrt}{\mathsf{sqrt}}
\newcommand{\rppsqrtstep}{\mathsf{sqrt_{step}}}
\newcommand{\PRF}{\mathsf{PRF}}

% title
\title{A Formal Verification of Reversible Primitive Permutations}
\author{Giacomo Maletto}
\date{}

\begin{document}

\maketitle

\chapter{The definition}

\section{Reversible computing}

Reversible computing is a model of computation in which every process can be run backwards.
Simply put, in a reversible setting any program takes inputs and gives outputs (like usual), but can also go the other way around:
provided the output it can reconstruct the input.
In a mathematical sense, every function is expected to be invertible.

Why do we care about such a thing?

Firstly, having a programming language in which every function (or even a subset of functions) is reversible could lead to interesting and practical applications.

But we can also imagine reversible computers, in which the underlying architecture is inherently reversible:
Toffoli gates provide a way to do so.
The opposite of reversibility is loss of information, which (for thermodynamic reasons) leads to loss of energy and heat dissipation.
This means that a non-reversible gate dissipates energy each time information is discarded, while in principle a reversible computer wouldn't.

Lastly, reversible computing is directly related to quantum computing, as each operation in a quantum computer must be reversible.


\section{Reversible Primitive Permutations}

In the article we decided to formalize, the authors focus on providing a functional model of reversible computation.
They develop an inductively defined set of functions, called \textbf{Reversible Primitive Permutations} or \textbf{RPP},
which are expressive enough to represent all Primitive Recursive Functions -
that is to say, $\RPP$ is $\PRF$-complete (we talk about what this means in section ?).
Here is the definition that we will use:

\newpage

\begin{definition}[Reversible Primitive Permutations]
\label{rppdef}
The class of \textbf{Reversible Primitive Permutations} or $\RPP$ is the smallest subset of functions $\ZZ^n \to \ZZ^n$ satisfying the following conditions:
\begin{itemize}

\item
The $n$-ary \textbf{identity} $\rppId_n (x_1, \dots, x_n) = (x_1, \dots, x_n)$ belongs to $\RPP$, for all $n \in \NN$.
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{3-1}{\rppId_n} & x_1    \\
  \Vdots &                       & \Vdots \\
  x_n    &                       & x_n    \\
\end{NiceMatrix}\]
The meaning of these diagrams should be fairly obvious:
if the values on the left of a function are provided as inputs to that function, we get the values on the right as outputs.
\item
The \textbf{sign-change} $\rppNe (x) = -x$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppNe} & -x \\
\end{NiceMatrix}\]

\item
The \textbf{successor function} $\rppSu (x) = x+1$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppSu} & x+1 \\
\end{NiceMatrix}\]

\item
The \textbf{predecessor function} $\rppPr (x) = x-1$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{1-1}{\rppPr} & x-1 \\
\end{NiceMatrix}\]

\item
The \textbf{swap} $\rppSw (x, y) = (y, x)$ belongs to $\RPP$.
\[\begin{NiceMatrix}
  x & \bloch{2-1}{\rppSw} & y \\
  y &                     & x \\
\end{NiceMatrix}\]

\item
If $f : \ZZ^n \to \ZZ^n$ and $g : \ZZ^n \to \ZZ^n$ belongs to $\RPP$,
then the \textbf{series composition} $(f \rppCo g) : \ZZ^n \to \ZZ^n$ belongs to $\RPP$ and is such that:
\[(f \rppCo g) (x_1, \dots, x_n) = g (f (x_1, \dots, x_n)) = (g \circ f) (x_1, \dots, x_n).\]
We remark that $f \rppCo g$ means that $f$ is applied first, and then $g$, in opposition to the standard functional composition (denoted by $\circ$).
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{3-1}{f \rppCo g} & z_1    & \Block{3-1}{=} & x_1    & \bloch{3-1}{f} & y_1    & \bloch{3-1}{g} & z_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots &                & \Vdots \\
  x_n    &                         & z_n    &                & x_n    &                & y_n    &                & z_n    \\
\end{NiceMatrix}\]

\item

If $f : \ZZ^n \to \ZZ^n$ and $g : \ZZ^m \to \ZZ^m$ belongs to $\RPP$,
then the \textbf{parallel composition} $(f \rppPa g) : \ZZ^{n + m} \to \ZZ^{n + m} $ belongs to $\RPP$ and is such that:
\[(f \rppPa g) (x_1, \dots, x_n, y_1, \dots, y_m) = (f (x_1, \dots, x_n), g (y_1, \dots, y_m)).\]
\[\begin{NiceMatrix}[nullify-dots]
  x_1    & \bloch{6-1}{f \rppPa g} & w_1    & \Block{6-1}{=} & x_1    & \bloch{3-1}{f} & w_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots \\
  x_n    &                         & w_n    &                & x_n    &                & w_n    \\
  y_1    &                         & z_1    &                & y_1    & \bloch{3-1}{g} & z_1    \\
  \Vdots &                         & \Vdots &                & \Vdots &                & \Vdots \\
  y_m    &                         & z_m    &                & y_m    &                & z_m    \\
\end{NiceMatrix}\]

\item
If $f : \ZZ^n \to \ZZ^n$ belongs to $\RPP$,
then then \textbf{finite iteration} $\rppIt[f] : \ZZ^{n + 1} \to \ZZ^{n + 1}$ belongs to $\RPP$ and is such that:
\[ \rppIt[f] (x, x_1, \dots, x_n) = (x, (\overbrace{f \circ \dots \circ f}^{\downarrow x \text{ times}}) (x_1, \dots, x_n)) \]
where $\downarrow (\cdot) : \ZZ \to \NN$ is defined as
\[\downarrow x = \begin{cases} x, & \text{if $x \ge 0$} \\
                               0, & \text{if $x < 0$} \end{cases}.\]
This means that the function $f$ is applied $\downarrow x$ times to $(x_1, \dots, x_n)$.
\[\begin{NiceMatrix}[nullify-dots]
  x      & \bloch{4-1}{\rppIt[f]} & x      & \Block{4-1}{=} & x      &                                                                       &                    &                & x      \\  
  x_1    &                        & y_1    &                & x_1    & \bloch{3-1}{f}                                                        & \Block{3-1}{\dots} & \bloch{3-1}{f} & y_1    \\
  \Vdots &                        & \Vdots &                & \Vdots &                                                                       &                    &                & \Vdots \\
  x_n    &                        & y_n    &                & x_n    &                                                                       &                    &                & y_n    \\
         &                        &        &                &        & \Block{1-3}{\underbrace{\hspace{5.5em}}_{\downarrow x \text{ times}}} &                    &                &        \\
\end{NiceMatrix}\]

\item
If $f, g, h : \ZZ^n \to \ZZ^n$ belong to $\RPP$,
then the \textbf{selection} $\rppIf[f, g, h] : \ZZ^{n + 1} \to \ZZ^{n + 1}$ belongs to $\RPP$ and is such that:
\[\rppIf[f, g, h] (x, x_1, \dots, x_n) = \begin{cases} (x, f (x_1, \dots, x_n)), & \text{if $x > 0$} \\
                                                    (x, g (x_1, \dots, x_n)), & \text{if $x = 0$} \\
                                                    (x, h (x_1, \dots, x_n)), & \text{if $x < 0$} \end{cases}.\]
We remark that the argument $x$ which determines which among $f$, $g$ and $h$ must be used cannot be among the arguments of $f$, $g$ and $h$,
as that would break reversibility.
\[\begin{NiceMatrix}[nullify-dots]
  x      & \bloch{4-1}{\rppIf[f, g, h]} & x      \\
  x_1    &                              & y_1    \\
  \Vdots &                              & \Vdots \\
  x_n    &                              & y_n    \\
\end{NiceMatrix} \quad
\begin{NiceMatrix}[nullify-dots]
                 &                     &                  \\
  \Block{3-1}{=} & f (x_1, \dots, x_n) & \text{if } x > 0 \\
                 & g (x_1, \dots, x_n) & \text{if } x = 0 \\
                 & h (x_1, \dots, x_n) & \text{if } x < 0 \\
\CodeAfter
\SubMatrix\}{2-1}{4-1}\{  
\end{NiceMatrix}\]

\end{itemize}
\end{definition}

\begin{remark}
\label{different_arity}
If we have two functions of different arity, for example $f : \ZZ^3 \to \ZZ^3$ and $g : \ZZ^5 \to \ZZ^5$,
then we will still write $f \rppCo g$ to mean the function with arity $\max(3, 5)=5$ given by $(f \rppPa \rppId_2) \rppCo g$.
In general, the arity of the "smaller" function can be enlarged by a suitable parallel composition with the identity.
The same goes for the arguments of the selection $\rppIf[f, g, h]$.
\[\begin{NiceMatrix}[nullify-dots]
  x_1 & \bloch{5-1}{f \rppCo g} & z_1 & \Block{5-1}{=} & x_1 & \bloch{3-1}{f} & y_1 & \bloch{5-1}{g} & z_1 & \Block{5-1}{=} & x_1 & \bloch{3-1}{f}        & y_1 & \bloch{5-1}{g} & z_1 \\
  x_2 &                         & z_2 &                & x_2 &                & y_2 &                & z_2 &                & x_2 &                       & y_2 &                & z_2 \\
  x_3 &                         & z_3 &                & x_3 &                & y_3 &                & z_3 &                & x_3 &                       & y_3 &                & z_3 \\
  x_4 &                         & z_4 &                & x_4 &                & x_4 &                & z_4 &                & x_4 & \bloch{2-1}{\rppId_2} & x_4 &                & z_4 \\
  x_5 &                         & z_5 &                & x_5 &                & x_5 &                & z_5 &                & x_5 &                       & x_5 &                & z_5 \\
\end{NiceMatrix}\]
\end{remark}


\section{Some examples}

In order to get accustomed to this definition, let's see some examples.

\paragraph{Increment and decrement}
Let's try to imagine what addition should look like in $\RPP$.
Of course, addition is usually thought of as a function which takes two inputs and yields their sum:
something like $\texttt{add}(x,y) = x+y$.
But notice that this operation is not reversible:
given only the output (the value $x+y$) it is impossible to obtain the original values ($x, y$).
As we will see, every function in $\RPP$ is reversible, so we will not be able to define addition in this way.

Instead, we can define a function $\rppinc$ in $\RPP$ which, given $n \in \NN$ and $x \in \ZZ$, yields
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppinc} & n     \\
  x &                      & x + n \\
\end{NiceMatrix}\]
If $n$ is negative the output is just $(n, x)$.
The fact that the above diagram is only valid for $n \in \NN$ might bother some of you;
we'll explain later why it is so, and how we can also make it work for $n \in \ZZ$.

For now let's focus on the output: we don't just have $x + n$ but also $n$, and indeed,
given both $n$ and $x+n$ we can reconstruct $n$ (obviously) and $x$ (by $(x+n)-n$).
As a matter of fact, the following function $\rppdec$ also belongs to $\RPP$:
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppdec} & n     \\
  x &                      & x - n \\
\end{NiceMatrix}\]
and if we try to compose $\rppinc$ and $\rppdec$ we get this remarkable result:
\[\begin{NiceMatrix}
  n & \bloch{2-1}{\rppinc} & n     & \bloch{2-1}{\rppdec} & n \\
  x &                      & x + n &                      & x \\
\end{NiceMatrix}\]
and similarly for $\rppdec \rppCo \rppinc$.
So indeed $\rppdec$ is the inverse of $\rppinc$, and we can write $\rppdec = \rppinc^{-1}$.

But we haven't said how to actually define $\rppinc$.
Well, just like this:
\[\rppinc = \rppIt[\rppSu]\]
This means that we apply the successor function $\rppSu$ to the value $x$, for $\downarrow n$ times.
If $n \in \NN$ then $\downarrow n = n$, so we effectively add $n$ to the value $x$.
If instead $n$ is negative then $\downarrow n = 0$ and nothing changes.

Can you guess how $\rppdec$ is defined?
\newpage
In a very similar manner, using the predecessor function:
\[\rppdec = \rppIt[\rppPr]\]
and as we will shortly see, finding the inverse is not something that we have to do by hand.

\paragraph{Multiplication and square}
We now turn our attention to multiplication.
The elementary-school way to define multiplication is by repeated addition, and we can define $\rppmul$ exactly like that:
\[\rppmul = \rppIt[\rppinc].\]
As $\rppinc$ had arity $2$, $\rppmul$ has arity $2+1=3$.
If $n, m \in \NN$ and $x \in \ZZ$ then we have
\[\begin{NiceMatrix}
  n & \bloch{3-1}{\rppmul} & n             \\
  m &                      & m             \\
  x &                      & x + n \cdot m \\
\end{NiceMatrix}\]
because we're essentially "incrementing by $m$" $n$ times;
so in this case we preserve both inputs and increase a certain variable $x$.

What is the inverse $\rppmul^{-1}$? Does it perform division?
Well, the truth is rather disappointing:
\[\begin{NiceMatrix}
  n & \bloch{3-1}{\rppmul^{-1}} & n             \\
  m &                           & m             \\
  x &                           & x - n \cdot m \\
\end{NiceMatrix}\]
We will see a way to calculate division in $\RPP$, but this is not it.

We're now ready to define the function $\rppsquare$ which is used to calculate the square of a number:
\[\rppsquare = (\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw).\]
That might look like a very complicated expression;
thankfully we can make use of diagrams to show what each step does.
Given $n \in \NN$ and $x \in \ZZ$ we have
\[\begin{NiceMatrix}
  n &                     & n & \bloch{2-1}{\rppinc} & n & \bloch{3-1}{\rppmul} & n             & \bloch{2-1}{\rppdec} & n             &                     & n             \\
  x & \bloch{2-1}{\rppSw} & 0 &                      & n &                      & n             &                      & 0             & \bloch{2-1}{\rppSw} & x + n \cdot n \\
  0 &                     & x &                      & x &                      & x + n \cdot n &                      & x + n \cdot n &                     & 0             \\
\end{NiceMatrix}\]
so we add the result $n \cdot n$ to a variable $x$;
we also require an additional value initialized to 0.
We will make frequent use of variables initially set to 0 and which come back to 0 after the calculation;
these are traditionally called \textbf{ancillary arguments} or \textbf{ancillaes}, from the latin term used to describe female house slaves in ancient Rome.

You might be wondering what would happen if $n < 0$ or the ancilla was different from $0$.
The truth is, we don't really care.
We will often specify the behaviour of these functions given some initial values,
and we won't need to know what happens for different initial values because we'll never use those functions in other ways.

\section{Calculating the inverse}

Earlier we hinted at the fact that every function in $\RPP$ is invertible and the inverse belongs to $\RPP$;
furthermore, we don't need to perform the calculation manually, case by case.
In other words, there is an \textit{effective procedure} which produces the inverse $f^{-1} \in \RPP$ given any $f \in \RPP$.

\begin{proposition}[The inverse $f^{-1}$ of any $f$]
\label{rppinv}
Let $f : \ZZ^n \to \ZZ^n$ belong to $\RPP$.
Then the inverse $f^{-1} : \ZZ^n \to \ZZ^n$ exists, belongs to $\RPP$ and, by definition, is
\begin{itemize}
\item $\rppId_n^{-1} = \rppId_n$
\item $\rppNe^{-1} = \rppNe$
\item $\rppSu^{-1} = \rppPr$
\item $\rppPr^{-1} = \rppSu$
\item $\rppSw^{-1} = \rppSw$
\item $(f \rppCo g)^{-1} = g^{-1} \rppCo f^{-1}$
\item $(f \rppPa g)^{-1} = f^{-1} \rppPa g^{-1}$
\item ${\rppIt[f]}^{-1} = \rppIt[f^{-1}]$
\item ${\rppIf[f, g, h]}^{-1} = \rppIf [f^{-1}, g^{-1}, h^{-1}]$
\end{itemize}
Then $f \rppCo f^{-1} = \rppId_n$ and $f^{-1} \rppCo f = \rppId_n$.
\end{proposition}
\begin{proof}
By induction on the definition of $f$.
\end{proof}

Well, that was rather succinct.

We invite the reader to check that every listed inverse does indeed make sense;
for example, the function $\rppIt[f] (x, y_1, \dots, y_n)$ applies $\downarrow x$ times the function $f$ to the argument $(y_1, \dots, y_n)$.
If we want to "undo" this effect we just need to apply $\downarrow x$ times $f^{-1}$ to the same argument, so
${\rppIt[f]}^{-1} = \rppIt[f^{-1}]$.

Of course, that reasoning only works if in turn $f$ is also invertible and $f^{-1} \in \RPP$.
This is the reason that the proof is by induction:
given an arbitrary $\RPP$, if we unfold one step of the definition we get one of the cases listed.
We apply the appropriate step and then by inductive hypothesis we can assume that in turn its sub-terms are invertible.

Notice that in this way, if we try to calculate $\rppinc^{-1}$ we really do get $\rppdec$, as we had hoped for.

Since $\RPP$ is inductively defined, any proposition involving $\RPP$ functions can be proven using induction.
Not only that, but any function which has for an argument a generic $\RPP$ can be defined recursively,
and indeed we can also see $(\cdot)^{-1}:\RPP \to \RPP$ as a recursive function.
Now that we delve into the Lean theorem prover we will see that induction and recursion can be seen as really the same thing,
and that's just one of many similarities between functions and proofs.

\section{First steps with Lean}

In this section we take a look at some of Lean's basic features.
You don't have to understand every detail -
just enough to have a vague sense of what it's like to define stuff in Lean.

\paragraph{}

In Lean we primarily do three things:
\begin{enumerate}
\item define data structures
\item define functions
\item prove theorems about data structures and functions
\end{enumerate}
What sets Lean apart from your average functional programming language (like Haskell) is the third item on the list.
Now we will instead focus on the first and second points.

You don't have to understand every detail of what will follow -
a vague understanding of what's going on would be sufficient.
The curious reader can run and play with most of the following snippets of code
in the online editor \url{https://leanprover-community.github.io/lean-web-editor/}.

\paragraph{A simple example of a type}
Data is defined using the \lstinline{inductive} keyword.
Here is the typical example of data structure:
\begin{lstlisting}
inductive weekday : Type
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday
| sunday : weekday
\end{lstlisting}
This defines a \textit{type} called \lstinline{weekday}.
Days of the week like \lstinline{monday}, \lstinline{tuesday}, etc. are elements of the type \lstinline{weekday}.
We can see the type of an element by using the \lstinline{#check} command:
\begin{lstlisting}
-- opening the scope weekday (otherwise to refer
-- to an element - for example tuesday - of weekday
-- we have to write weekday.tuesday)
open weekday

#check tuesday -- this outputs "weekday"
\end{lstlisting}
and we write this as
\begin{center}\begin{lstlisting}
tuesday : weekday
\end{lstlisting}\end{center}
Everything in Lean has a type (and only one). For example, natural numbers have type \lstinline{ℕ}:
\begin{lstlisting}
#check 3 -- ℕ
\end{lstlisting}
Even type themselves have a type\footnote{Types in Lean have a role similar to sets in math.
Standard math axioms (like ZFC) dictates that everything is a set, including sets themselves.
This basic notion can lead to some contradictory statements, like the famous Russell's paradox
(let's consider the set of all sets that do not contain themselves; does this set contain itself?)
and if one is not careful in defining types of types, the same thing could happen with type theory.
But in fact, type theory was invented in the beginning of the 20th century by Bertrand Russell precisely to avoid Russell's paradox.
The approach used in Lean is to define a cumulative hierarchy of universes \lstinline{Type : Type 1 : Type 2 ...},
so that it's impossible to invoke objects like "the type of all types" or a type having itself as an element.}.
Lean's type system is very expressive, and makes it possible to work with complex math in Lean.

We can define functions over the type \lstinline{weekday} -
for example, the function \lstinline{next}:

\begin{lstlisting}
-- Special characters like → will abound.
-- In VS Code and the Lean Web Editor,
-- arrows can be inserted by typing \to and hitting
-- the space bar. 
def next : weekday → weekday
| monday    := tuesday
| tuesday   := wednesday
| wednesday := thursday
| thursday  := friday
| friday    := saturday
| saturday  := sunday
| sunday    := monday

#reduce next wednesday -- this outputs "thursday"
#check next -- next has type "weekday → weekday"
\end{lstlisting}
This function is defined by cases:
if we have \lstinline{monday}, output \lstinline{tuesday},
if we have \lstinline{tuesday}, output \lstinline{wednesday}, and so on.

(Almost) every expression - like \lstinline{next (next thursday)} or \lstinline{3 * 5 + 2} -
have a corresponding \textit{reduced form} (respectively \lstinline{saturday} and \lstinline{17})
which can be displayed using the \lstinline{#reduce} command,
and is obtained by repeatedly applying functions to their arguments, until the full computation is carried out.
In this sense, things like \lstinline{next wednesday} and \lstinline{next (next tuesday)}
(or \lstinline{2 + 2} and \lstinline{1 + 3}) are \textit{definitionally equivalent}, because they're reduced to the same expression.

An important remark on notation:
in math it is customary to call functions by enclosing arguments in parenthesis and separating them with commas, i.e. $f(x,y,z)$.
Languages like Lean follow a different convention: the arguments are simply written after the function name, like \lstinline{f x y z}.
So in our case, what we would write as $\mathsf{next(next(thursday))}$ is instead written \lstinline{next (next thursday)}
(writing \lstinline{next next thursday} would be wrong
because it would mean that the first argument to \lstinline{next} is the function \lstinline{next} itself, not \lstinline{next thursday}).
This leads to no ambiguity and often helps reducing clutter.

\paragraph{An example of an inductive type}
Right now you could be wondering why we used the keyword \lstinline{inductive} to define \lstinline{weekday},
when there's \textit{clearly} no induction going on at all in its construction.
First of all, it depends on what you mean by induction; but it is true that that was a particularly simple case.
As an example of a more overtly inductive object, we can define the natural numbers like this:
\begin{lstlisting}
inductive Nat : Type
| Zero : Nat
| Succ (n : Nat) : Nat
\end{lstlisting}
The name \lstinline{Nat} and subsequent objects are capitalized in order to avoid conflict with the definition of \lstinline{nat} already present in Lean.
We can read this definition as "every element of the type \lstinline{Nat} is either \lstinline{Zero}
or \lstinline{Succ n} where \lstinline{n : Nat}",
which is basically the Peano definition of natural numbers.
Some examples of elements of this type:
\begin{lstlisting}
open Nat

-- all these outputs "Nat"
#check Zero -- represents 0
#check Succ Zero -- represents 1
#check Succ (Succ Zero) -- represents 2
#check Succ (Succ (Succ Zero)) -- represent 3
#check Zero.Succ.Succ.Succ -- also represents 3
                           -- alternative notation
\end{lstlisting}
Functions over \lstinline{Nat} have the possibility of being truly recursive:
for example, we can recursively define addition \lstinline{Add m n} by induction over \lstinline{n}.
\begin{itemize}
\item if \lstinline{n = Zero} then \lstinline{Add m Zero = m}
\item if \lstinline{n = Succ n'} for some \lstinline{n' : Nat}, \\
      then \lstinline{Add m (Succ n') = Succ (Add m n')}.
\end{itemize}
Note that by definition each element of \lstinline{Nat} can be either \lstinline{Zero} or \lstinline{Succ n'} for some \lstinline{n' : Nat},
so the two cases considered cover all possibilities.
Written in Lean,
\begin{lstlisting}
def Add : Nat → Nat → Nat
| m Zero      := m
| m (Succ n') := Succ (Add m n')
\end{lstlisting}
It may have struck you that the type of \lstinline{Add} is not \lstinline{Nat × Nat → Nat}
but instead \lstinline{Nat → Nat → Nat}.
This is known as currying, and it's not as strange as it might look like at first.
Consider this: we can think of \lstinline{Add} as a function which takes a pair \lstinline{(m,n) : Nat × Nat} and outputs \lstinline{Add m n : Nat},
as is standard in mathematics.
But we can also think of it as a function which takes just \lstinline{m : Nat} and outputs the function \lstinline{Add m : Nat → Nat},
which in turn given \lstinline{n : Nat} outputs \lstinline{Add m n : Nat}.
We can think of \lstinline{Add m} as a partially applied function, which becomes fully applied when it is given another argument \lstinline{n}.
From this point of view, \lstinline{Add} is a function of type \lstinline{Nat → (Nat → Nat)} which is the same as
\lstinline{Nat → Nat → Nat} because in Lean the arrow \lstinline{→} is right associative.

In a certain sense, currying makes functions conceptually simpler;
all functions are single variable, it's just that some return other functions.

\paragraph{Integers and lists}
Functions belonging to $\RPP$ have $\ZZ^n$ as their domain and codomain,
so we need a way to represent and work with integer tuples.

The good news is that integers are already defined in Lean.
Here is their definition:
\begin{lstlisting}
inductive int : Type
| of_nat (n : ℕ) : int
| neg_succ_of_nat (n : ℕ) : int  
\end{lstlisting}
The value \lstinline{of_nat n} represents the natural number \lstinline{n : ℕ} as an integer,
while \lstinline{neg_succ_of_nat n} represents the negative number \lstinline{-(n+1)}.
Of course it's not the definition we've just seen that gives this meaning to the \lstinline{int}s;
rather, it's the functions defined on them (like addition, subtraction etc.).

Immediately after the definition, some notation is introduced:
\begin{itemize}
\item \lstinline{ℤ} stands for \lstinline{int}
\item in a context in which an integer is expected, if instead a natural number is supplied,
the function \lstinline{of_nat} will be automatically applied on the natural number.
This convenient feature is called coercion.
\item for every \lstinline{n : ℕ}, \lstinline{-[1+ n]} stands for \lstinline{neg_succ_of_nat n}.
This notation is almost never used.
\end{itemize}
\begin{lstlisting}
notation `ℤ` := int

instance : has_coe nat int := ⟨int.of_nat⟩

notation `-[1+ ` n `]` := int.neg_succ_of_nat n
\end{lstlisting}
As an example of a function from \lstinline{ℤ} to \lstinline{ℤ}, this is negation:
\begin{lstlisting}
def neg : ℤ → ℤ
| (of_nat n) := neg_of_nat n
| -[1+ n]    := succ n
\end{lstlisting}

We're interested not just in integers, but in tuples of integers.
We can implement the concept of a tuple in many ways,
but a particularly simple one is through the use of lists,
a very common data structure in computer science.

Let's consider lists of natural numbers, i.e. the type \lstinline{list ℕ}.
This is a list of 5 elements:
\begin{lstlisting}
open list
#reduce [4, 5, 7, 2, 5] -- [4, 5, 7, 2, 5]
\end{lstlisting}
The first element of a list is the \textit{head}
\begin{lstlisting}
#reduce head [4, 5, 7, 2, 5] -- 4
\end{lstlisting}
while the other elements are the \textit{tail}.
\begin{lstlisting}
#reduce tail [4, 5, 7, 2, 5] -- [5, 7, 2, 5]
\end{lstlisting}
Given \lstinline{n : ℕ} and \lstinline{l : list ℕ} we can obtain a new list
\lstinline{cons n l} (also written as \lstinline{n :: l}) such that
\lstinline{head (n :: l) = n} and \lstinline{head (n :: l) = l}
\begin{lstlisting}
#reduce cons 2 [4, 5, 7, 2, 5] -- [2, 4, 5, 7, 2, 5]
#reduce 2 :: [4, 5, 7, 2, 5]   -- alternative notation  
\end{lstlisting}
and ultimately, every list can be obtained by starting with \lstinline{nil},
the empty list, and repeatedly using \lstinline{cons}.
\begin{lstlisting}
#reduce nil -- the empty list
#reduce []  -- alternative notation 
#reduce cons 4 (cons 5 (cons 7 (cons 2 (cons 5 nil))))
            -- [4, 5, 7, 2, 5]
#reduce 4 :: 5 :: 7 :: 2 :: 5 :: []
            -- alternative notation
\end{lstlisting}
This might suggest a definition of lists of naturals:
a \lstinline{list_nat} is either the empty list \lstinline{nil_nat},
or \lstinline{cons_nat hd tl} where \lstinline{hd : ℕ} and \lstinline{tl : list_nat}
are respectively the head and tail of the list:
\begin{lstlisting}
inductive list_nat : Type
| nil_nat : list_nat
| cons_nat (hd : ℕ) (tl : list_nat) : list_nat
\end{lstlisting}
There's nothing special about using natural numbers.
We can use the same procedure to define lists of integers:
\begin{lstlisting}
inductive list_int : Type
| nil_int : list_int
| cons_int (hd : ℤ) (tl : list_int) : list_int
\end{lstlisting}
but having to define different types of lists for each type of element is pretty cumbersome.
Instead, we can define lists for a generic type \lstinline{T} using \textit{dependent types}:\footnote{a Lean user would probably frown at this,
because it would be best to choose an explicit universe \lstinline{u} and work with \lstinline{Type u}.}
\begin{lstlisting}
inductive list (T : Type) : Type
| nil : list
| cons (hd : T) (tl : list) : list
\end{lstlisting}
Rather than having \lstinline{list_nat}, \lstinline{list_int}...
we use \lstinline{list ℕ}, \lstinline{list ℤ}...
\lstinline{list α} where \lstinline{α} is any type.

We can see how useful dependent types are by defining the function \lstinline{length}
which returns the number of elements of a list:
\begin{lstlisting}
def length {α : Type*} : list α → ℕ
| []       := 0
| (a :: l) := length l + 1
\end{lstlisting}
Note that we can use \lstinline|{α : Type*}| to refer to a generic type \lstinline{α}.
If instead we had stuck to \lstinline{list_nat}, \lstinline{list_int}...
now we would have to define \lstinline{length_nat}, \lstinline{length_int}... separately for each type.

We will identity tuples of $n$ elements $\ZZ^n$ with lists in \lstinline{list ℤ} of length \lstinline{n}.

\section{The definition in Lean}

\paragraph{Syntax and semantics}
Let's now ask ourselves: how can we define in a satisfactory way the class of functions $\RPP$ in Lean,
using just types and functions?
We'd like to be able to do proofs by induction over $\RPP$, like in proposition \ref{rppinv}, so we'll need to define an inductive type.

The key is thinking about $\RPP$ not as a class of functions, but as a small programming language.
In this sense, we can write down "programs" like our square function
\[(\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw)\]
but we should not view it only as a function belonging to $\ZZ^3 \to \ZZ^3$,
but also as the sentence
"$(\rppId_1 \rppPa \rppSw) \rppCo \rppinc \rppCo \rppmul \rppCo \rppdec \rppCo (\rppId_1 \rppPa \rppSw)$"
which can then be interpreted as the mathematical function belonging to $\ZZ^3 \to \ZZ^3$.

We thus separate between the \textit{syntax} and the \textit{semantics} of our language.
\begin{itemize}
\item The syntax are the rules which governs how to assemble well-structured sentences.
For example, the selection symbol $\rppIf$ should be followed by three other $\RPP$ functions;
if we write $\rppIf[\rppSu, \rppPr] \rppCo \rppNe$ we get a non-valid sentence.
\item The semantics is the meaning we give to (well-structured) sentences -
in our case, they are intepreted as functions $\ZZ^n \to \ZZ^n$.
\end{itemize}

A possible way to define $\RPP$ functions in Lean is
\begin{itemize}
\item define the type $\RPP$ which has for elements syntactically-correct sentences of $\RPP$
\item define a function $\mathsf{evaluate} : \RPP \to (\ZZ^n \to \ZZ^n)$ which assigns to each $\RPP$-sentence its intended meaning,
namely a function $\ZZ^n \to \ZZ^n$.
\end{itemize}
Note that this is not the only way in which this task can be accomplished;
we will discuss other methods in section ?.

We thus define the type $\RPP$ as follows:
\begin{lstlisting}
inductive RPP : Type
| Id (n : ℕ) : RPP
| Ne : RPP
| Su : RPP
| Pr : RPP
| Sw : RPP
| Co (f g : RPP) : RPP
| Pa (f g : RPP) : RPP
| It (f : RPP) : RPP
| If (f g h : RPP) : RPP
\end{lstlisting}
and also introduce custom notation:
\begin{lstlisting}
-- the numbers 50 and 55 denote the precedence -
-- simply put, Ne ;; Su ‖ Pr is intepreted as
-- Ne ;; (Su ‖ Pr), not (Ne ;; Su) ‖ Pr
infix `;;` : 50 := Co
infix `‖` : 55 := Pa
\end{lstlisting}
so it's now possible to write expressions like
\begin{lstlisting}
#check It Su ;; (Id 2 ‖ If Sw Pr Su) -- RPP
\end{lstlisting}
Remember that by remark \ref{different_arity}, it makes sense to consider the series composition of functions of different arity,
as long as we give them the meaning specified in the remark.

Talking about arity, how do we deal with it?
In order to define $\mathsf{evaluate}$ and give meaning to $\RPP$,
we must be able to define a concept of arity,
otherwise we'll have trouble with parallel composition of two functions \lstinline{f ‖ g} -
the arity of \lstinline{f} must be known,
otherwise it's impossible to tell what to apply \lstinline{g} to.

Luckily, we can reconstruct the arity of an \lstinline{RPP} just by looking at its symbolic representation:
\begin{lstlisting}
def arity : RPP → ℕ
| (Id n)     := n
| Ne         := 1
| Su         := 1
| Pr         := 1
| Sw         := 2
| (f ;; g)   := max f.arity g.arity
| (f ‖ g)    := f.arity + g.arity
| (It f)     := f.arity + 1
| (If f g h) := max (max f.arity g.arity) h.arity + 1
\end{lstlisting}
Note that \lstinline{f.arity} is the same as \lstinline{(arity f)}.
This is a recursive function:
there are 5 base cases and in the other 4 the value of \lstinline{arity} is reconstructed from smaller sub-terms.

It's now possible to define some \lstinline{RPP}-sentences in Lean
\begin{lstlisting}
def Id₁ := Id 1
def inc := It Su
def dec := It Pr
def mul := It inc
def square := Id₁ ‖ Sw ;; inc ;; mul ;; dec ;; Id₁ ‖ Sw
\end{lstlisting}
and it's even possible to calculate their arity
\begin{lstlisting}
#reduce square.arity -- outputs "3"
\end{lstlisting}
but we haven't yet given their meaning as functions.

\paragraph{The \lstinline{ev} function}
We are now ready to define \lstinline{evaluate} (\lstinline{ev} for short).
The function $\mathsf{ev}$ should take $\RPP$-sentences and return functions $\ZZ^n \to \ZZ^n$,
so in Lean we will define it as a function of type
\begin{lstlisting}
RPP → (list ℤ → list ℤ)
\end{lstlisting}
which in Lean is the same as
\begin{lstlisting}
RPP → list ℤ → list ℤ.
\end{lstlisting}
Here's how we do it:
\begin{lstlisting}
def ev : RPP → list ℤ → list ℤ
| (Id n)     l                    := l
| Ne         (x :: l)             := -x :: l
| Su         (x :: l)             := (x + 1) :: l
| Pr         (x :: l)             := (x - 1) :: l
| Sw         (x :: y :: l)        := y :: x :: l
| (f ;; g)   l                    := ev g (ev f l)
| (f ‖ g)    l                    := ev f (take f.arity l) ++
                                     ev g (drop f.arity l)
| (It f)     (x :: l)             := x :: ((ev f)^[↓x] l)
| (If f g h) (0 :: l)             := 0 :: ev g l
| (If f g h) (((n : ℕ) + 1) :: l) := (n + 1) :: ev f l
| (If f g h) (-[1+ n] :: l)       := -[1+ n] :: ev h l
| _          l                    := l

notation `‹` f `›` := ev f
\end{lstlisting}
We will write \lstinline{‹f›} to mean the function of type \lstinline{list ℤ → list ℤ} given by \lstinline{ev f}.

Here's a case-by-case analysis:
\begin{itemize}
\item \lstinline{‹Id n› l} is the original list \lstinline{l}, unchanged.
\item \lstinline{‹Ne› (x :: l)} reduces to \lstinline{-x :: l}, which is same list but with the head of opposite sign.
\item \lstinline{‹Su› (x :: l)} reduces to the same list but with the head incremented by one.
\item \lstinline{‹Pr› (x :: l)} reduces to the same list but with the head decremented by one.
\item \lstinline{‹Sw› (x :: y :: l)} reduces to the same list but with the first two elements swapped.
\item \lstinline{‹f ;; g› l} successively applies \lstinline{‹f›} and \lstinline{‹g›} to the list.
\item \lstinline{‹f ‖ g› l} applies \lstinline{‹f›} to the first \lstinline{f.arity} elements of the list,
applies \lstinline{‹g›} to the remaining elements of the list,
and then joins the two parts through \lstinline{append} (which is the \lstinline{(++)} operator).
\item \lstinline{‹It f› (x :: l)} leaves the head unchanged and applies \lstinline{‹f›} to the tail \lstinline{↓x} times,
where \lstinline{↓x} is defined as in definition \ref{rppdef}.
\item \lstinline{‹If f g h› (0 :: l)} leaves the head unchanged and applies \lstinline{‹g›} to the tail.
\item \lstinline{‹If f g h› (((n : ℕ) + 1) :: l)} is the case where the head is a positive number
(a natural number plus \lstinline{1}),
and as such leaves the head unchanged and applies \lstinline{‹f›} to the tail.
\item \lstinline{‹If f g h› (-[1+ n] :: l)} is the case where the head is a negative number,
and as such leaves the head unchanged and applies \lstinline{‹h›} to the tail.
\item In all cases not considered (for example, applying \lstinline{‹Ne›} to an empty list) the whole list remains unchanged. 
\end{itemize}

The reader is invited to compare this definition with the one given in definition \ref{rppdef}.

Let's see some examples:
\begin{lstlisting}
#check ‹It Su ;; (Id₁ ‖ If Sw Pr Su)› -- list ℤ → list ℤ
-- #eval is similar to #reduce
-- but in this case gives more readable output
#eval ‹inc› [3, 4] -- [3, 7]
#eval ‹square› [19, 0, 0] -- [19, 361, 0]
\end{lstlisting}
It magically works. We finally have our definition formalized in Lean.

It is worth noting that even though lists supplied to \lstinline{‹f›} are supposed to have length equal to \lstinline{f.arity},
this is never enforced.
So we are free to apply \lstinline{‹f›} to a list which is too short or too long.
If it's too short, unspecified things will happen, we don't care.
If it's too long, only the first \lstinline{f.arity} items are utilized and affected,
and this is guaranteed by theorem \lstinline{ev_split} which we will prove in Lean.
So, when we apply \lstinline{RPP} functions to a list, we'll have to make sure that
\lstinline{f.arity ≤ l.length}.

\paragraph{The \lstinline{inverse} function}
It's not hard to convert our proposition \ref{rppinv} into a function definition:
\lstinline{inv : RPP → RPP} which given \lstinline{f : RPP} returns its inverse.
\begin{lstlisting}
def inv : RPP → RPP
| (Id n)     := Id n
| Ne         := Ne
| Su         := Pr
| Pr         := Su
| Sw         := Sw
| (f ;; g)   := inv g ;; inv f
| (f ‖ g)    := inv f ‖ inv g
| (It f)     := It (inv f)
| (If f g h) := If (inv f) (inv g) (inv h)

notation f `⁻¹` := inv f
\end{lstlisting}
Now it's possible to define \lstinline{dec} simply as \lstinline{inc⁻¹}.

We will also prove in Lean that \lstinline{f⁻¹} really is the inverse (in the functional sense) of \lstinline{f},
but it will require some work.
\section{Differences with the original definition}
The definition of \lstinline{RPP} functions we've given differs quite a bit from the original one.
Every change has been made in the name of simplicity:
theorem proving in Lean is hard enough,
we don't need to make it harder by choosing inconvenient definitions.
Below is a list of changes, not only for completeness' sake but also to illustrate the kind of reasoning which goes on
when formalizing definitions in Lean.
\begin{itemize}
\item In the original definition,
in the iterator $\rppIt$ and selection $\rppIf$ the last element of the tuple is checked,
not the first one (the head).
It was more convenient to work with the first element because of the definition of lists:
it's much easier to consider a list's head and tail than its last element and the elements before the last.
\item We have defined $\rppId_n$ as a $n$-ary function, while originally it was just unary.
Having a $n$-ary identity function is very useful,
because we can use parallel composition as in remark \ref{different_arity},
and also because we have the possibility of having a $0$-ary function, which is not useless in some cases.
\item The original $\RPP$ functions are defined as the union $\cup_{n \in \NN}\RPP^n$
where $\RPP^n$ are the $n$-ary $\RPP$ functions.
A similar decision could've been made in Lean by definining \lstinline{RPP n} as a dependent type
with parameter \lstinline{n : ℕ},
but it turned out that it was possible to calculate the arity of an \lstinline{RPP} simply by looking at
the corresponding \lstinline{RPP}-sentence, which is what we did when we defined the function \lstinline{arity}.
This rendered superfluous using dependent types and separating \lstinline{RPP} based on their arity.

There's a reason we tried to avoid dependent types wherever possible
(which also led to the use of \lstinline{list}s instead of \lstinline{vector}s):
at least in Coq (which is another proof assistants we used at the beginning of the project)
working with dependent types is often painful, because Coq doesn't recognize that certain types are the same.
For example, elements of \lstinline{RPP (n + 1)} and \lstinline{RPP (1 + n)} cannot be compared even though it is (demonstrably!) true that
\lstinline{n + 1 = 1 + n}. To get around this, it's possible to use something called John Major's Equality
to state the equality of two objects with seemingly different types,
but this involves the invocation of an additional axiom and is in general annoying to use.
Other ways to deal with the problem exist,
but our choice ended up being avoiding dependent types completely.
As someone on the internet says,
\begin{displayquote}
Coq has this really powerful type system, but... don't use it.
\end{displayquote}
By extension, we also avoided them in Lean.
\item When defining the iterator $\rppIt[f] (x, x_1, \dots, x_n)$ it's not immediately clear what to do when $x < 0$.
In our definition, nothing happens, as $f$ in general is applied $\downarrow x = 0$ times.
In the original definition, $f$ is instead applied $| x |$ times - let's call this iterator $\rppIta$.

Reversibility gifts us with a third option: if $x < 0$,
we can apply $f$ a negative amount of times - or in other words, we can apply $f^{-1}$ for $- x$ times.
Let's call this iterator $\rppItr$.
Its usage leads to more natural definitions:
for example, our function $\rppinc (n, x) = \rppIt[\rppSu] (n, x)$ returns $(n, x+n)$ only if $n \ge 0$.
If instead we use $\rppItr$, suddenly $\rppItr[\rppSu] (n, x) = (n, x + n)$ for all values of $n \in \ZZ$.

So why didn't we use $\rppItr$?
Because our $\rppIt$ is the most versatile option:
we can define both $\rppIta$ and $\rppItr$ in terms of $\rppIt$,
by using the fact that $\rppIt$ doesn't do anything when the first argument is negative:
\begin{align*}
  \rppIta[f] = \rppIt[f] \rppCo \rppNe \rppCo \rppIt[f] \rppCo \rppNe \\
  \rppItr[f] = \rppIt[f] \rppCo \rppNe \rppCo \rppIt[f^{-1}] \rppCo \rppNe
\end{align*}
For example, in the case of $\rppIta[f](x,x_1,\cdots,x_n)$, if $x \ge 0$ then the first $\rppIt$ applies $f$ for $x$ times,
then $x$ changes sign and becomes $-x$ with $\rppNe$, then the second $\rppIt$ doesn't do anything because $- x < 0$,
and finally $-x$ changes sign again to $x$; if instead $x < 0$, only the second $\rppIt$ does something.

Another reason to prefer $\rppIt$ over $\rppItr$ is that in the definition of \lstinline{ev},
using $\rppItr$ it's hard to convince Lean (or Coq) that the function terminates (that is, it doesn't run on an infinite loop).
Since every function in Lean must terminate (otherwise there would be consistency issues), Lean rejects the definition.
There are ways to get around this - but once again we follow the path of least resistance and just get on with $\rppIt$.
\end{itemize}

After seeing all these changes you might ask yourself - is this still the original $\RPP$?
What's the point of formalizing a definition in Lean if in the process we change the definition completely?

We think that yes, we can still identify what we've constructed as the original functions,
because in a way, the \textit{essence} of what $\RPP$ is has not been altered:
a class of functions which is reversible by construction and that is $\PRF$-complete.
We shouldn't view definitions as something unchanging and rigid, especially in rapidly evolving fields.
Definitions should be molded and modified to fit our needs,
because that's why we created them in the first place.


\chapter{Theorem proving}

Here we finally delve into the main characteristic which distinguishes Lean from usual programming languages:
the possibility of formally proving results about the objects defined.

\section{Some examples of Lean proofs}

\paragraph{Reflexivity}

We define the type \lstinline{Nat} similarly as before
\begin{lstlisting}
inductive Nat : Type
| O : Nat -- it's a capital o, not a zero
| S (n : Nat) : Nat

open Nat
#reduce O -- represents 0
#reduce S O -- represents 1
#reduce O.S -- also represents 1
            -- we'll use this notation
#reduce O.S.S -- represents 2

\end{lstlisting}
together with addition
\begin{lstlisting}
def Add : Nat → Nat → Nat
| m O      := m
| m (S n') := (Add m n').S  

-- if n m : Nat then n + m is defined as Add n m
infix `+` := Add
\end{lstlisting}
  
Now let's prove some theorems about these objects.
Let's start with something simple: we want to prove,
beyond the shadow of a doubt, that \lstinline{O = O}.
We open our code editor and type this:
\begin{lstlisting}
lemma O_eq_O : O = O :=
begin

end
\end{lstlisting}
If we now place the cursor between \lstinline{begin} and \lstinline{end}
this appears in the sidebar:
\begin{lstlisting}
  1 goal
  
  ⊢ O = O
\end{lstlisting}
This is called the \textbf{tactic state}.
The line beginning with the turnstile \lstinline{⊢ O = O} is our \textbf{goal}.
We can write commands called \textbf{tactics} which help us solve goals.
In this case, the goal is an equality in which the left-hand side happens to coincide perfectly with the right-hand side,
so we can solve our goal using the \lstinline{refl} command
\begin{lstlisting}
lemma O_eq_O : O = O :=
begin
  refl,
end
\end{lstlisting}
Placing the cursor just after the comma, a pleasant message appears:
\begin{lstlisting}
  goals accomplished
\end{lstlisting}
The name \lstinline{refl} stands for "reflexivity",
which is a property of equality (for any \lstinline{a}, it is true that \lstinline{a = a}).
Let's try something more involved: $2 + 2 = 4$.
\begin{lstlisting}
lemma two_plus_two : O.S.S + O.S.S = O.S.S.S.S :=
begin

end
\end{lstlisting}
This time the left-hand side and the right-hand side do not look identical.
However, there's something interesting to note:
\begin{lstlisting}
#reduce O.S.S.S.S -- outputs "O.S.S.S.S"
#reduce O.S.S + O.S.S -- also outputs "O.S.S.S.S"
\end{lstlisting}
that is, \lstinline{O.S.S + O.S.S} reduces to \lstinline{O.S.S.S.S}.
Since the left-hand side and the right-hand side reduce to the same element,
they are \textbf{definitionally equivalent} and so we can use the tactic \lstinline{refl} again:
\begin{lstlisting}
lemma two_plus_two : O.S.S + O.S.S = O.S.S.S.S :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}
Remember that by definition of \lstinline{Add}, for any \lstinline{n m : Nat},
we have that \lstinline{n + m.S = (n + m).S}.
We can express this with another theorem
\begin{lstlisting}
lemma plus_S (n m : Nat) : n + m.S = (n + m).S :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}
which again can be solved using \lstinline{refl}, because \lstinline{n + m.S} reduces to \lstinline{(n + m).S}.
By the way, we could write \lstinline{theorem} instead of \lstinline{lemma}: the difference is only stylistic.
Similarly, by definition of \lstinline{Add}, \lstinline{n + O = n} for all \lstinline{n : Nat}
\begin{lstlisting}
lemma plus_O (n : Nat) : n + O = n :=
begin
  refl, -- goals accomplished
end
\end{lstlisting}

\paragraph{Induction and rewrite}

If instead we try to prove in the same way that \lstinline{O + n = n},
\begin{lstlisting}
def O_plus (n : Nat) : O + n = n :=
begin
  refl,
end
\end{lstlisting}
something surprising happens:
\begin{lstlisting}
invalid apply tactic, failed to unify
    O+n = n
  with
    ?m_2 = ?m_2
  state:
  n : Nat
  ⊢ O+n = n
\end{lstlisting}
our trusted \lstinline{refl} has, alas, failed.
This is because \lstinline{O + n} is \textbf{not} definitionally equivalent to \lstinline{n}:
the function \lstinline{Add} defines two definitional equivalences
(\lstinline{m + O = m} and \lstinline{m + n.S = (m + n).S})
and there's nothing regarding \lstinline{0 + n} when we have a generic \lstinline{n : Nat}.
However, two things can still be equal even if they are not definitionally equivalent.

To prove \lstinline{O_plus} we need something stronger: the tactic \lstinline{induction}.
Let's try it:
\begin{lstlisting}
def O_plus (n : Nat) : O + n = n :=
begin
  induction n using n hn,
end
\end{lstlisting}
Now the tactic state has become
\begin{lstlisting}
  2 goals

  case Nat.O
  ⊢ O+O = O

  case Nat.S
  n: Nat
  hn: O+n = n
  ⊢ O+n.S = n.S
\end{lstlisting}
What happened is that we used induction:
to prove a property \lstinline{P n} (in this case, \lstinline{O + n = n}) for all \lstinline{n : Nat},
it suffices to prove that \lstinline{P O} holds and that \lstinline{P n} implies \lstinline{P n.S}.
The first subgoal is the base case \lstinline{O+O = O}, and can be solved using \lstinline{refl}
\begin{lstlisting}
lemma O_plus (n : Nat) : O + n = n :=
begin
  induction n with n hn,
  refl,
end
\end{lstlisting}
Only the second goal remains
\begin{lstlisting}
  1 goal

  case Nat.S
  n : Nat
  hn : O+n = n
  ⊢ O+n.S = n.S
\end{lstlisting}
This means that \lstinline{n} is an element of \lstinline{Nat}
and that we have an hypothesis named \lstinline{hn} which tells us that \lstinline{O+n = n}.
Our goal is to prove that \lstinline{O+n.S = n.S}.
A proof of this fact would go somewhat like this:
\begin{enumerate}
\item by lemma \lstinline{plus_S} we have \lstinline{O+n.S = (O+n).S}
\item by the induction hypothesis \lstinline{hn} we have \lstinline{O+n = n} and by substitution we get \lstinline{(0+n).S = n.S}
\end{enumerate}
so \lstinline{O+n.S = (O+n).S = n.S}, and this completes the proof.

We can capture this act of substituting a term in an equation with an equivalent one using the tactic \lstinline{rw} ("rewrite"):
for example, \lstinline{rw plus_S} search in the goal for subterms of the form \lstinline{O+n.S} and substitutes them with \lstinline{(O+n).S}.
More generally, given an equality \lstinline{h : a = b}, calling \lstinline{rw h} substitutes occurrences of \lstinline{a} in the goal with \lstinline{b}.
\begin{lstlisting}
lemma O_plus (n : Nat) : O + n = n :=
begin
  induction n with n hn,
  refl,
             -- goal: O+n.S = n.S
  rw plus_S, -- goal: (O+n).S = n.S
  rw hn,     -- goal: n.S = n.S
             -- refl is automatically called with rw,
             -- so: goals accomplished!
end
\end{lstlisting}
Let's tackle one more theorem: the commutativiy of addition
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin

end
\end{lstlisting}
Again, using \lstinline{refl} doesn't work, so we use induction.
We have a choice: using induction on \lstinline{n} or \lstinline{m};
note that doing induction on one or the other is not the same,
because \lstinline{n} and \lstinline{m} have asymmetric roles in the definition of \lstinline{Add}.
In particular, the second argument gets "broken down" at each step
(since \lstinline{n + m.S = (n + m).S}) while the first argument doesn't change.
Thus, in this case the best choice is induction on \lstinline{m}.
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin
  induction m with m hm, -- 2 goals
                         -- 
                         -- case Nat.O
                         -- n: Nat
                         -- ⊢ n+O = O+n
                         --
                         -- case Nat.S
                         -- nm: Nat
                         -- hm: n+m = m+n
                         -- ⊢ n+m.S = m.S+n
end
\end{lstlisting}
We can deal with the base case \lstinline{n+O = O+n} by using \lstinline{rw} with our two lemmas
\lstinline{plus_O : n + O = n} and \lstinline{O_plus : O + n = n}:
\begin{lstlisting}
  rw plus_O, rw O_plus, -- first goal vanquished
\end{lstlisting}
So now we have hypothesis \lstinline{hm : n+m = m+n} and goal \lstinline{n+m.S = m.S+n}.
We can use \lstinline{rw plus_S} to change the goal to \lstinline{(n+m).S = m.S+n},
and if we could further rewrite it to \lstinline{(n+m).S = (m+n).S} then we would use the hypothesis
to solve the goal.
Problem is, we don't have a theorem which states that \lstinline{m.S+n = (m+n).S} - but we leave it as an exercise for the reader.
Having called it \lstinline{S_plus} we can thus conclude our proof
\begin{lstlisting}
lemma plus_comm (n m : Nat) : n + m = m + n :=
begin
  induction m with m hm,
  rw plus_O, rw O_plus,
  rw plus_S, rw S_plus, rw hm, -- goals accomplished
end
\end{lstlisting}
A remark: since the rules \lstinline{n + m.S = (n + m).S} and \lstinline{n + O = n} are exactly the definition of \lstinline{Add},
we don't actually need to write \lstinline{rw plus_S} and \lstinline{rw plus_O}, we can instead use \lstinline{rw Add}
which includes both these equalities.

After seeing some examples of tactics-based proofs,
you might come to the conclusion that they are very unreadable and difficult to understand.
That's not entirely false, but it's important to notice that the interactive component of Lean is vital to its usage:
just reading tactics it's practically impossible to get what's going on, especially for more involved proofs.
On the other hand seeing the hypotheses, the goals, and how they change at each steps of the proof
immensely clarifies the process of understanding and usage.

\paragraph{}

So, we've now learned some basics about theorem proving in Lean,
but we don't know anything yet about what proofs \textit{are} and how they fit in the general scheme of things.
There is a lot to be learned.

\section{Curry-Howard correspondence}

The following section is not necessary to understand the rest of the thesis,
so the busy reader can skip it.

In Lean, things like propositions and proofs are not completely separated from data objects like types and elements of types.
We previously stated that in Lean, everything has a type, and we can see what type a certain object has by using the \lstinline{#check} command.

So, let's feed random stuff to \lstinline{#check}.
\begin{lstlisting}
#check O -- Nat
#check Nat -- Type
#check Type -- Type 1
#check Type 1 -- Type 2
              -- it's an infinite family of types
              -- for each u, Type u is an element of Type (u+1)
#check Add -- Nat → Nat → Nat
#check Nat → Nat → Nat -- Type

-- some theorem names...
#check two_plus_two -- O.S.S+O.S.S = O.S.S.S.S
#check O_plus -- ∀ (n : Nat), O+n = n
#check plus_comm -- ∀ (n m : Nat), n+m = m+n

-- ..and their statements
#check O.S.S+O.S.S = O.S.S.S.S -- Prop
#check ∀ (n : Nat), O+n = n -- Prop
#check ∀ (n m : Nat), n+m = m+n -- Prop

#check Prop -- Type
\end{lstlisting}
So, there is a type called \lstinline{Prop} and its elements are propositions.
Any proposition, like \lstinline{∀ (n : Nat), O+n = n}, is in turn a type - but what exactly are its elements?

The elements of a proposition are proofs of that proposition.
This means that what we have called \lstinline{two_plus_two} is a proof of the fact that \lstinline{O.S.S + O.S.S = O.S.S.S.S}.
What we mean by proving a proposition, is finding an element whose type is that proposition.
For example, here is the definition of the proposition \lstinline{true}:
\begin{lstlisting}
inductive true : Prop -- this is a Prop, not a Type
| intro : true    
\end{lstlisting}
How do we know that \lstinline{true} is true? Because it has an element, \lstinline{true.intro}
(another way to say it is that \lstinline{true.intro} is a proof of the proposition \lstinline{true}):
\begin{lstlisting}
lemma true_is_true : true :=
begin
  exact true.intro, -- if we have an explicit element
                    -- of the proposition to be proven,
                    -- we can use the tactic exact
end
\end{lstlisting}

The proposition \lstinline{false} is defined like this:
\begin{lstlisting}
inductive false : Prop  
\end{lstlisting}
It's a type with no elements, so \lstinline{false} can't be proven.

Suppose that $A$ is a proposition in classical logic.
Then it is true that $A \Rightarrow A$ (we can derive it using natural deduction, for example).
In Lean this fact can be expressed as the proposition \lstinline{A → A}, and it's something provable:
\begin{lstlisting}
lemma A_implies_A (A : Prop) : A → A :=
begin
  -- 1 goal
  -- A: Prop
  -- ⊢ A → A
end
\end{lstlisting}
At the left of the implication arrow we have \lstinline{A}:
we can thus turn \lstinline{A} into an hypothesis using the \lstinline{intro} tactic
\begin{lstlisting}
lemma A_implies_A (A : Prop) : A → A :=
begin
  intro h, -- creates a hypothesis h : A
  -- 1 goal
  -- A : Prop
  -- h : A
  -- ⊢ A
  exact h, -- goals accomplished
end
\end{lstlisting}
Let's forget for a moment that \lstinline{A} is a proposition, let's think of it as just a type.
Then something funny happens: we see that \lstinline{A → A} is just the type of functions from \lstinline{A} to \lstinline{A}.
If we can exhibit an element of this type, then we have proven that \lstinline{A → A}.
But this is easy enough: we can use the identity function
\begin{lstlisting}
def A_implies_A' (A : Prop) : A → A
| h := h    
\end{lstlisting}
and this proves the proposition, but also defines a function.
It can be interpreted like this:
if we have a proof \lstinline{h : A} and we have to prove \lstinline{A} then we can just exhibit \lstinline{h}; that's it.

Amazingly, it turns out all proofs are really just functions: we can see this using the \lstinline{#print} command,
which given a function prints out its definition.
\begin{lstlisting}
#print Add
  -- def Add : Nat → Nat → Nat :=
  -- λ (n m : Nat), Nat.rec n (λ (m' n_m' : Nat), n_m'.S) m
#print A_implies_A
  -- theorem A_implies_A : ∀ (A : Prop), A → A :=
  -- λ (A : Prop) (h : A), h
#print O_plus
  -- theorem O_plus : ∀ (n : Nat), O+n = n :=
  -- λ (n : Nat),
  --   Nat.rec (eq.refl (O+O))
  --     (λ (n : Nat) (hn : O+n = n),
  --        (id (eq.rec (eq.refl (O+n.S = n.S))
  --                    (plus_S O n))).mpr
  --          ((id (eq.rec (eq.refl ((O+n).S = n.S)) hn)).mpr
  --            (eq.refl n.S)))
  --     n
\end{lstlisting}
When we proved \lstinline{O_plus} we didn't explicitly write a function,
we used tactics, but it's important to notice that the sequence of tactics \textit{isn't} the proof -
instead, tactics generate proofs (functions).
To further illustrate this point, notice that we can also define the function \lstinline{Add} using tactics:
\begin{lstlisting}
def Add_tactic (n m : Nat) : Nat :=
begin
  -- 1 goal
  -- n m : Nat
  -- ⊢ Nat
  -- to "solve" the goal we have to provide a Nat
  induction m with m' n_m', -- induction on m
  exact n, -- base case, m=O: we return n
  exact n_m'.S, -- inductive case, Add n m' = n_m'
                -- we have to provide Add n m'.S:
                -- we return n_m'.S
                -- "goals accomplished"
end

#reduce Add_tactic O.S.S O.S.S.S.S -- O.S.S.S.S.S.S
\end{lstlisting}
And notice! \lstinline{Add} is a recursive function, so to define it we had to use induction.
Going back to \lstinline{#print Add} and \lstinline{#print O_plus},
we can see that both call a function called \lstinline{Nat.rec}. Let's investigate:
\begin{lstlisting}
#check Nat.rec
  -- Nat.rec : ?M_1 O →
  --           (Π (n : Nat), ?M_1 n → ?M_1 n.S) →
  --           Π (n : Nat), ?M_1 n
\end{lstlisting}
If we intepret \lstinline{?M_1} as a proposition \lstinline{P : Nat → Prop} which ranges over a \lstinline{Nat},
then \lstinline{Nat.rec} reads off as
"if \lstinline{P O} holds true and for all \lstinline{n : Nat},
\lstinline{P n} implies \lstinline{P n.S}, then \lstinline{P n} holds for all \lstinline{n : Nat}" which is the principle of induction.
If instead we interpret \lstinline{?M_1} as a function \lstinline{f : Nat → T} with domain \lstinline{Nat} and codomain a certain type \lstinline{T},
then \lstinline{Nat.rec} reads off as
"if we define \lstinline{f O} and for all \lstinline{n : Nat},
given \lstinline{f n} we define \lstinline{f n.S}, then we have defined a function \lstinline{Nat → T}" which is how we define recursive functions.

\lstinline{Nat.rec} is called the \textbf{induction principle} of \lstinline{Nat},
and is auto-generated as soon as \lstinline{Nat} is defined.
Every type generates an induction principle - even simple types like our earlier \lstinline{weekday} -
hence every type is defined using the \lstinline{inductive} keyword.
The same induction principle is used both for recursive functions and inductive proofs.

The remarkable thing about Lean is that the concepts of types and propositions, functions and proofs are united in a single mechanism,
which results in a particularly simple foundation for mathematics and computing.
This concept is called the \textbf{Curry-Howard correspondence} and it's not something unique to Lean -
it's a common characteristic of many theorem prover, especially those based on (intuitionistic) type theory.

\section{The simplification tactic}

A hot topic in proof assistants is automatization.
For many, being able to generate automatically new theorems and new math is the ultimate objective of theorem provers.
Certainly Lean has value even before such a feat is accomplished,
but where automation is already avaiable it would be a waste not to use it.

A simple tool which helps tremendously with theorem proving is the \lstinline{simp} tactic.
It automaticaly tries to apply already known theorems in order to simplify a given expression.
We illustrate this with an example:
\begin{lstlisting}
lemma many_adds (n : Nat) :
  (O + (O + (n.S + (n.S + (O + n))))) = (n + (n + n)).S.S :=
begin
  rw O_plus,
  rw O_plus,
  rw O_plus,
  rw S_plus,
  rw S_plus,
  rw plus_S, -- goals accomplished
end
\end{lstlisting}
Using our previous lemmas it's not hard to prove this,
but there's a lot of repetition and any slight change to the statement probably results in some misapplied tactics.
Equalities like \lstinline{O_plus : O + n = n} makes our expression strictly simpler,
so usually there wouldn't be any reason to not rewrite it automatically.
We can do such a thing by marking the theorem \lstinline{O_plus} with the \lstinline{@[simp]} tag
\begin{lstlisting}
@[simp] lemma O_plus (n : Nat) : O + n = n :=
begin
  ...  
\end{lstlisting}
and then using the \lstinline{simp} tactic in the proof of \lstinline{many_adds}:
\begin{lstlisting}
lemma many_adds (n : Nat) :
  (O + (O + (n.S + (n.S + (O + n))))) = (n + (n + n)).S.S :=
begin
  simp, -- 1 goal
        -- n : Nat
        -- ⊢ n.S+(n.S+n) = (n+(n+n)).S.S
end
\end{lstlisting}
If we also mark lemmas \lstinline{plus_S} and \lstinline{S_plus} with \lstinline{@[simp]},
then we can conclude \lstinline{many_adds} with a single use tactic:
\begin{lstlisting}
lemma many_adds (n : Nat) :
  (O + (O + (n.S + (n.S + (O + n))))) = (n + (n + n)).S.S :=
begin
  simp, -- goals accomplished
end  
\end{lstlisting}
We can mark with \lstinline{@[simp]} theorems which have an equality or a bi-implication as thesis.
Notice that we put \lstinline{@[simp]} besides theorems that we want to be later utilized by \lstinline{simp},
not in theorems where we want to use the tactic \lstinline{simp}.

We can also mark definitions and functions,
so that direct consequences of the the definition are automatically simplified.
For example, marking \lstinline{Add} with \lstinline{@[simp]} is the same as marking \lstinline{plus_O} and \lstinline{plus_S}. 

This tactic has some more functionalities: if we want to mark certain theorems or definitions \lstinline{th1, th2, def1, ...}
just for one \lstinline{simp} call, we can write \lstinline{simp[th1, th2, def1, ...]} and we can also mark every local hypothesis
by using the asterisk \lstinline{simp[*]}.
Finally, hypotheses themselves can be the target of simplification:
if we want to simplify a hypothesis \lstinline{h} we write \lstinline{simp at h};
if we want to simplify all hypotheses and the goal, we write \lstinline{simp at *}.

Using \lstinline{simp} and marking theorems often starts off an avalanche effect:
each new theorem makes \lstinline{simp} stronger, which helps it prove new theorems.
In the following section we will make heavy use of \lstinline{simp} on theorems about \lstinline{RPP}.

It's usually not a good idea to indiscriminately mark every equality with \lstinline{@[simp]}, however.
That's because \lstinline{simp} is very eager to apply every theorem it can each time it has the opportunity.
This means that if we mark something like our \lstinline{add_comm : ∀ (n m : Nat), n+m = m+n}
then when \lstinline{simp} meets a sum \lstinline{n+m}, it gets stuck in an infinite loop,
endlessly "simplifying" it to \lstinline{m+n} and back to \lstinline{n+m}.
Hence, when we tag an equality as \lstinline{@[simp]},
it's good practice to make sure that the right-hand side is strictly simpler than the left-hand side,
and that no infinite loops can occur.

\section{Basic theorems about RPP}

In the previous chapter we found a meaningful way to express $\RPP$ in Lean.
Our next objective is proving the most important property of this class of functions:
each $f \in \RPP$ admits an inverse $f^{-1} \in \RPP$.

This result was discussed in proposition \ref{rppinv},
and we already constructed the function \lstinline{inv : RPP → RPP} which given an \lstinline{RPP} returns its syntactical inverse.
What remains to be done is to show that this \textit{really} provides the inverse function,
i.e. that \lstinline{‹f ;; f⁻¹› l = ‹f⁻¹ ;; f› l = l} for all \lstinline{f : RPP} and \lstinline{l : list ℤ}.

To be honest, we could settle for something easier:
since we decided that we don't really care about what happens when \lstinline{l.length < f.arity},
we could put \lstinline{f.arity ≤ l.length} as an hypothesis, making our theorem slightly weaker.
However, we will not do this, because it'd mean that at each use of the theorem we'd have to supply that hypothesis;
and because the way in which we defined \lstinline{RPP} allows us to state the result in full generality.

This is the statement:
\begin{lstlisting}
-- proposition 1 expressed with RPP composition,
-- about the left inverse
theorem proposition_1_co_l (f : RPP) (l : list ℤ) :
  ‹f ;; f⁻¹› l = l
\end{lstlisting}
We also need to prove \lstinline{proposition_1_co_r} about the right inverse \lstinline{‹f⁻¹ ;; f› l = l},
but there's a clever way to derive this fact from \lstinline{proposition_1_co_l}
and a lemma called \lstinline{inv_involute} which will be our first result about \lstinline{RPP} formalized in Lean:
\begin{lstlisting}
@[simp] lemma inv_involute (f : RPP) : (f⁻¹)⁻¹ = f :=
by { induction f; simp * } -- by { ... } is equivalent to
                           -- begin ... end
\end{lstlisting}
We mark this lemma with \lstinline{@[simp]} because it is an equality where the right-hand side is strictly simpler than the left-hand side.
Using this fact, to prove that
\begin{lstlisting}
‹f⁻¹ ;; f› l = l
\end{lstlisting}
we rewrite it as
\begin{lstlisting}
‹f⁻¹ ;; (f⁻¹)⁻¹› l = l
\end{lstlisting}
and then apply theorem \lstinline{proposition_1_co_l} with argument \lstinline{f⁻¹}.

We still have to prove that theorem, though.
Let's follow in the footsteps of proposition \ref{rppinv} and begin by induction on f:
\begin{lstlisting}
theorem proposition_1_co_l (f : RPP) (l : list ℤ) :
  ‹f ;; f⁻¹› l = l :=
begin
  induction f,
end
\end{lstlisting}
This generates not less than 9 goals!
It's something to be expected, because the inductive definition of \lstinline{RPP} is composed of 9 cases.
Let's study each one in excruciating detail.

\paragraph{The base cases}

Seeing all 9 goals at each step is stressful;
let's use curly braces to limit our view to just the current goal:
\begin{lstlisting}
begin
  induction f,
  {  }, -- putting the cursor between the curly braces shows
        --   1 goal
        --   case RPP.Id
        --   l : list ℤ
        --   n : ℕ
        --   ⊢ ‹Id n;;Id n⁻¹› l = l
end
\end{lstlisting}
We're in luck: by definition of \lstinline{ev} and \lstinline{inv},
\lstinline{‹Id n;;Id n⁻¹› l} is definitionally equivalent to \lstinline{l},
so \lstinline{refl} solves the goal
\begin{lstlisting}
  { refl, }, -- goals accomplished
\end{lstlisting}

Now we just have 8 left.
Here's the second one:
\begin{lstlisting}
  case RPP.Ne
  l: list ℤ
  ⊢ ‹Ne;;Ne⁻¹› l = l
\end{lstlisting}
Sadly, just shoving in \lstinline{refl} like before doesn't do much good.
We should proceed by cases:
\begin{itemize}
\item The simplest case is when \lstinline{l} is the empty list \lstinline{[]}.
When this happens, the definitional equality \lstinline{‹Ne› (x :: l) = -x :: l} doesn't apply because \lstinline{l} is not of the form \lstinline{x :: l'}.
Instead, we fall back to the last case considered in the definition of \lstinline{ev}, that is: the whole list remains unchanged.
As a consequence, by \lstinline{refl} we get that
\begin{lstlisting}
‹Ne;;Ne⁻¹› l = ‹Ne⁻¹› (‹Ne› []) = [] = l.
\end{lstlisting}

\item If \lstinline{l} is not the empty list, i.e. \lstinline{l = x :: l'} for some \lstinline{x : ℤ} and \lstinline{l' : list ℤ},
then by our definitions
\begin{lstlisting}
‹Ne;;Ne⁻¹› l = ‹Ne⁻¹› (‹Ne› (x :: l')) = ‹Ne⁻¹› (-x :: l')
             = ‹Ne› (-x :: l') = -(-x) :: l'
             = x :: l' = l
\end{lstlisting}
The tactic \lstinline{refl} would \textit{almost} solve this,
but doesn't because \lstinline{-(-x)} is not definitionally equivalent to \lstinline{x}.
However, that equality is proven elsewhere and is marked as \lstinline{@[simp]}.
For \lstinline{simp} to fully work, it must also use the definitions of \lstinline{inv} and \lstinline{ev}.
Since the functions \lstinline{inv} and \lstinline{arity} depends only on the syntax and are generally pretty easy to calculate,
we mark these two functions as \lstinline{@[simp]}, so that they are always simplified:
\begin{lstlisting}
@[simp] def inv : RPP → RPP
  ...

@[simp] def arity : RPP → ℕ
  ...
\end{lstlisting}
but we \textbf{do not} do the same for \lstinline{ev}, because it's a considerably more complicated function
which also depends on the list that the \lstinline{RPP} function is applied to.
When we want \lstinline{simp} to simplify \lstinline{ev} (like in this case), we will explictly write \lstinline{simp [ev]}.
\end{itemize}
How do we consider separately the cases when \lstinline{l} is of the form \lstinline{x :: l'} or \lstinline{[]}?
By using the tactics
\lstinline{cases}:\footnote{Deep down, \lstinline{cases} is just an alternative to \lstinline{induction} that doesn't generate inductive hypotheses.}
\begin{lstlisting}
{ cases l with x l', -- splits into 2 goals
  refl,              -- case when l = []
  simp [ev],         -- case when l = x :: l'
                     -- goals accomplished
},
\end{lstlisting}
We've succesfully solved another goal.
The cases \lstinline{Su}, \lstinline{Pr}, \lstinline{Sw} are very similar and we won't discuss them further.

\paragraph{Series composition}

After those we get \lstinline{Co}:
\begin{lstlisting}
  case RPP.Co
  l : list ℤ
  f g : RPP
  hf : ‹f;;f⁻¹› l = l
  hg : ‹g;;g⁻¹› l = l
  ⊢ ‹f;;g;;(f;;g)⁻¹› l = l
\end{lstlisting}
Rather than jumping straight to Lean,
it's very often helpful to sketch proofs on pen and paper, to get an idea of how to proceed:
\begin{lstlisting}
‹f;;g;;(f;;g)⁻¹› l = ‹f⁻¹› (‹g⁻¹› (‹g› (‹f› l)))
                   = ‹f⁻¹› (‹g;;g⁻¹› (‹f› l))
             (!!!) = ‹f⁻¹› (‹f› l)
                   = ‹f;;f⁻¹› l
                   = l
\end{lstlisting}
There's something worrying:
we can't apply step \lstinline{(!!!)} because the inductive hypothesis \lstinline{hg} is too weak,
it works only if the applied list is precisely \lstinline{l}, while in this case it is \lstinline{‹f› l}.
The proper induction hypothesis would be something like this:
\begin{lstlisting}
∀ (l : list ℤ), ‹g;;g⁻¹› l = l
\end{lstlisting}
and Lean provides a functionality precisely for these instances:
at the beginning of the proof, it's sufficient to substitute \lstinline{induction f} with \lstinline{induction f generalizing l}.

After this is done, the \lstinline{simp} tactics makes short work of the goal:
\begin{lstlisting}
{ simp [ev, *] at *, -- simplifies every hypothesis and the goal,
                     -- using ev and each hypothesis.
                     -- goals accomplished
},
\end{lstlisting}

\paragraph{Parallel composition}

Parallel composition turns out to be the the most troublesome case.
\begin{lstlisting}
  case RPP.Pa
  l : list ℤ
  f g : RPP
  hf : ‹f;;f⁻¹› l = l
  hg : ‹g;;g⁻¹› l = l
  ⊢ ‹f‖g;;(f‖g)⁻¹› l = l
\end{lstlisting}
This is what we'd like to do:
\begin{lstlisting}
‹f‖g;;(f‖g)⁻¹› l = ‹f‖g;;f⁻¹‖g⁻¹› l
           (!!!) = ‹(f;;f⁻¹)‖(g;;g⁻¹)› l
                 = ‹f;;f⁻¹› (take f.arity l) ++
                   ‹g;;g⁻¹› (drop f.arity l)
                 = take f.arity l ++ drop f.arity l
                 = l
\end{lstlisting}
The tactic \lstinline{simp} is able to take care of each step, except for \lstinline{(!!!)},
which is not obvious at all and must be proven:
\begin{lstlisting}
lemma pa_co_pa (f f' g g' : RPP) (l : list ℤ) :
  f.arity = f'.arity →
  ‹f ‖ g ;; f' ‖ g'› l = ‹(f ;; f') ‖ (g ;; g')› l
\end{lstlisting}
The proof of this fact is rather tedious, partly because there are no hypotheses on the length of the list \lstinline{l},
so multiple cases have to be considered.

There is, however, the hypothesis \lstinline{f.arity = f'.arity}, without which the lemma is false.
Returning to the main proof, we can use our newly proven \lstinline{pa_co_pa} together with \lstinline{simp} to prove the goal,
but Lean requires the hypothesis to be satisfied:
\begin{lstlisting}
  f.arity = f⁻¹.arity
\end{lstlisting}
Yet another lemma needs to be proven and used
\begin{lstlisting}
@[simp] lemma arity_inv (f : RPP) : f⁻¹.arity = f.arity :=
by { induction f; simp [*, max_comm] }
\end{lstlisting}
after which everything goes smoothly.

\paragraph{Iteration and selection}
This is the \lstinline{It} case:
\begin{lstlisting}
  case RPP.It
  f: RPP
  hf : ∀ (l : list ℤ), ‹f;;f⁻¹› l = l
  l : list ℤ
  ⊢ ‹It f;;It f⁻¹› l = l
\end{lstlisting}
Let's start by using \lstinline{cases l} to split the proof:
\begin{lstlisting}
  case list.nil RPP.It
  f: RPP
  hf : ∀ (l : list ℤ), ‹f;;f⁻¹› l = l
  ⊢ ‹It f;;It f⁻¹› [] = []

  case list.cons RPP.It
  f: RPP
  hf : ∀ (l : list ℤ), ‹f;;f⁻¹› l = l
  x : ℤ
  l' : list ℤ
  ⊢ ‹It f;;It f⁻¹› (x :: l') = x :: l'
\end{lstlisting}
The first case is trivially solvable using \lstinline{refl}.
In the second one, running \lstinline{simp} turns the goal state into
\begin{lstlisting}
  f: RPP
  x : ℤ
  l' : list ℤ
  hf : ∀ (l : list ℤ), ‹f⁻¹› (‹f› l) = l
  ⊢ ‹f⁻¹›^[↓x] (‹f›^[↓x] l') = l'
\end{lstlisting}
The notation \lstinline{‹f›^[↓x]} means "\lstinline{‹f›} applied \lstinline{↓x} times".
Our objective is essentially "given that \lstinline{‹f⁻¹›} is the left inverse of \lstinline{‹f›} (hypothesis \lstinline{hf}),
prove that \lstinline{‹f⁻¹›^[↓x]} is the left inverse of \lstinline{‹f›^[↓x]} (goal)".

This fact is true in general and luckily, it's a result already present in Mathlib:
\begin{lstlisting}
theorem left_inverse.iterate {g : α → α}
  (hg : left_inverse g f) (n : ℕ) :
  left_inverse (g^[n]) (f^[n])
\end{lstlisting}
where \lstinline{left_inverse} is a proposition defined like this:
\begin{lstlisting}
def left_inverse (g : β → α) (f : α → β) : Prop :=
  ∀ x, g (f x) = x
\end{lstlisting}
so applying \lstinline{function.left_inverse.iterate} solves our goal.
Hopefully it's clear how useful having an extensive underlying math library is when proving things:
most basic facts are already present, so we can concentrate on more specific matters.

\paragraph{}

Finally, the \lstinline{If} case is not that hard,
we just split it into many sub-cases:
\lstinline{l} is equal to \lstinline{[]} or \lstinline{x :: l'} with \lstinline{x : ℤ} and \lstinline{l' : list ℤ},
and \lstinline{x} is either \lstinline{0}, a positive or a negative number.
Each of the sub-goals is solved using \lstinline{simp}.

\paragraph{Wrapping it all up}
Well, that was rather prolix.

The resulting proof, however, is not so large
\begin{lstlisting}
theorem proposition_1_co_l (f : RPP) (l : list ℤ) :
  ‹f ;; f⁻¹› l = l :=
begin
  induction f generalizing l,
  { refl, },
  { cases l with x l', refl, simp [ev], },
  { cases l with x l', refl, simp [ev], },
  { cases l with x l', refl, simp [ev], },
  { cases l with x l', refl,
    cases l' with y l'', refl, refl, },
  { simp [ev, *] at *, },
  { rw [inv, pa_co_pa], simp [ev, *] at *, rw arity_inv, },
  { cases l with x l', refl, simp [ev] at *,
    exact function.left_inverse.iterate f_ih (↓x) l', },
  { cases l with x l', refl,
    cases x, cases x,
    simp [ev, *] at *, simp [ev, *] at *, simp [ev, *] at *, },
end
\end{lstlisting}
and by knowing some more tactics, it can be further shortened to
\begin{lstlisting}
theorem proposition_1_co_l (f : RPP) (l : list ℤ) :
  ‹f ;; f⁻¹› l = l :=
begin
  induction f generalizing l; cases l with x l,
  any_goals {simp [ev, *], done},
  { cases l with y l, refl, simp [ev] },
  { simp [ev, *] at * },
  { rw [inv, pa_co_pa], simp [ev, *] at *, rw arity_inv },
  { simp [ev] at *, apply function.left_inverse.iterate f_ih },
  { rcases x with ⟨n, n⟩; simp [ev, *] at * }
end
\end{lstlisting}
We've succesfully proved a non-trivial result in Lean.
It is possible to express it in a more convenient, \lstinline{simp}-friendly way, which will become useful later:
\begin{lstlisting}
@[simp] theorem proposition_1 (f : RPP) (l l' : list ℤ) :
  ‹f⁻¹› l = l' ↔ ‹f› l' = l
\end{lstlisting}

It's natural to compare the Lean proof to "the \LaTeX\space one" given in proposition \ref{rppinv}.
The traditional proof is short and ignores a lot of finer points;
the Lean one is considerably longer,
practically unreadable without looking at the proof state and absolutely precise in every detail.
They both have their strengths and weaknesses,
and what we will hopefully see in the future of mathematics is a joyful collaboration between these two types of data.

\paragraph{}

We state just two more general results:
the first is that \lstinline{RPP} functions don't alter the length of a list:
\begin{lstlisting}
@[simp] lemma ev_length (f : RPP) (l : list ℤ) :
  (‹f› l).length = l.length
\end{lstlisting}
The second is that applying \lstinline{‹f›} to a list \lstinline{l}
is equivalent to applying \lstinline{‹f›} to the first \lstinline{f.arity} elements of \lstinline{l}
and then appending the remaining ones, like this:
\begin{lstlisting}
theorem ev_split (f : RPP) (l : list ℤ) :
  ‹f› l = ‹f› (take f.arity l) ++ drop f.arity l
\end{lstlisting}
Marking this theorem as \lstinline{@[simp]} would be a very bad idea,
because the right-hand side is more complicated than the left one,
so an infinite loop would occur.

\newpage
\section{A library of functions}

In this section we show that many common functions have a \lstinline{RPP} counterpart.
Some arguments will be declared as natural numbers (usually denoted with variables \lstinline{n}, \lstinline{m}):
we do not care about the behaviour of these functions when a negative number is supplied in place of a natural number.

For reference, here's a summary of already defined functions:
\begin{lstlisting}
def Id₁ := Id 1
def inc := It Su
def dec := It Pr
def mul := It inc 
\end{lstlisting}
\[\begin{NiceMatrix}[nullify-dots]
  x & \bloch{1-1}{\rppId_1} & x \\
\end{NiceMatrix}
\hspace{3em}
\begin{NiceMatrix}
  n & \bloch{2-1}{\rppinc} & n     \\
  x &                      & x + n \\
\end{NiceMatrix}
\hspace{3em}
\begin{NiceMatrix}
  n & \bloch{2-1}{\rppdec} & n     \\
  x &                      & x - n \\
\end{NiceMatrix}
\hspace{3em}
\begin{NiceMatrix}
  n & \bloch{3-1}{\rppmul} & n             \\
  m &                      & m             \\
  x &                      & x + n \cdot m \\
\end{NiceMatrix}\]
\begin{lstlisting}
def square := Id₁ ‖ Sw ;; inc ;; mul ;; dec ;; Id₁ ‖ Sw 
\end{lstlisting}
\[\begin{NiceMatrix}
  n &                     & n & \bloch{2-1}{\rppinc} & n & \bloch{3-1}{\rppmul} & n             & \bloch{2-1}{\rppdec} & n             &                     & n             \\
  x & \bloch{2-1}{\rppSw} & 0 &                      & n &                      & n             &                      & 0             & \bloch{2-1}{\rppSw} & x + n \cdot n \\
  0 &                     & x &                      & x &                      & x + n \cdot n &                      & x + n \cdot n &                     & 0             \\
\end{NiceMatrix}\]

\paragraph{Rewiring permutations}
We call a function a \textit{rewiring permutation} if it only results in a finite permutation of the positions of its arguments.

We use the following notation: given $I = \{ i_0, \dots, i_n \} \subseteq \{ 0, \dots, m \}$ with all $i_k$ \textbf{distinct},
let $J = \{ j_1, \dots, j_{m-n} \} := \{ 0, \dots, m \} \setminus I$ be the set of remaining indices, ordered such that each $j_k < j_{k+1}$.
Then we define the rewiring permutation $\lfloor i_0, \dots, i_n \rceil$ to be the function such that
\[ \lfloor i_0, \dots, i_n \rceil (x_0, \dots, x_m) = (x_{i_0}, \dots, x_{i_n}, x_{j_1}, \dots, x_{j_{m-n}}) .\]
For example, $\lfloor 3, 1, 4 \rceil (a, b, c, d, e, f) = (d, b, e, a, c, f)$.
Of course, every rewiring permutation can be written in such a way.

To express this function in \lstinline{RPP} we begin by defining a function \lstinline{call n}
which moves the argument found in position \lstinline{n} to position \lstinline{0}, like so:
\begin{lstlisting}
def call : ℕ → RPP
| 0       := Id₁
| (i + 1) := Id i ‖ Sw ;; call i
\end{lstlisting}
We can see it as a long chain of swaps \lstinline{Sw}
that progressively move the argument in position \lstinline{n} to position \lstinline{n-1}, then \lstinline{n-2} etc.

The function \lstinline{rewire : list ℕ → RPP} takes a list of indices \lstinline{i₀, i₁, ..., iₙ} and returns an \lstinline{RPP}
that performs the permutation \lstinline{⌊i₀, i₁, ..., iₙ⌉}, through a clever repeated use of \lstinline{call}.
We do not mark \lstinline{rewire} as \lstinline{@[simp]} because it would lead to extremely messy goal states,
but instead mark every auxiliary function used in the definition of \lstinline{rewire},
so that when we do need to use \lstinline{simp},
we just have to write \lstinline{simp[rewire]} without having to also list all the auxiliary functions.
\[\begin{NiceMatrix}
  a & \bloch{6-1}{3\\ 1\\ 4} & d \\
  b &                        & b \\
  c &                        & e \\
  d &                        & a \\
  e &                        & c \\
  f &                        & f \\
\end{NiceMatrix}\]

\paragraph{A comparison function}

The following function takes two naturals \lstinline{n m : ℕ} and a number set to \lstinline{0},
and changes it to \lstinline{1} if \lstinline{n < m}.
The logic behind it is straightforward.

\begin{lstlisting}
def less := dec ;; Id₁ ‖ If Su Id₁ Id₁ ;; inc
\end{lstlisting}
\[\begin{NiceMatrix}
  \oloch{3-1}{\ n < m} & n & \bloch{2-1}{\rppdec} & n     &                                                             & n     & \bloch{2-1}{\rppinc} & n \\
                       & m &                      & m - n & \bloch{2-1}{\rppIf [\boldsymbol\rppSu, \rppId_1, \rppId_1]} & m - n &                      & m \\
                       & 0 &                      & 0     &                                                             & 1     &                      & 1 \\
\\
  \oloch{3-1}{\ n \ge m} & n & \bloch{2-1}{\rppdec} & n     &                                                                            & n     & \bloch{2-1}{\rppinc} & n \\
                         & m &                      & m - n & \bloch{2-1}{\rppIf [\rppSu, \boldsymbol{\rppId_1}, \boldsymbol{\rppId_1}]} & m - n &                      & m \\
                         & 0 &                      & 0     &                                                                            & 0     &                      & 0 \\
\end{NiceMatrix}\]

\paragraph{Division, Cantor unpairing \& square root}
Here we learn that division, Cantor unpairing and square root function are (almost) the same thing.

\paragraph{}

We define the \textbf{Cantor pairing} as the function $\rppcp : \NN^2 \to \NN$ such that
\[ \rppcp(x,y)= \sum_{i=1}^{x+y}i + x = \frac{(x + y)(x + y + 1)}{2} + x.\]
What's special about this function is that it's a bijection between $\NN^2$ and $\NN$
(\textit{maybe} the only polynomial to be so\footnote{It's currently an open question
whether there are other polynomials which are bijection $\NN^2 \to \NN$.}).
We can imagine it to be an enumeration of the square grid $\NN^2$:

\begin{figure}[H]
  \centering
  \includegraphics[width=15em]{Immagini/cp.png}
\end{figure}

But also as a "path" taken on the same grid, which traverses each point exactly once:

\begin{figure}[H]
  \centering
  \includegraphics[width=15em]{Immagini/cammino.png}
\end{figure}

Naturally, being bijective means that there exists an inverse function $\rppcu : \NN \to \NN^2$,
called the \textbf{Cantor unpairing}, which is usually expressed as
\[ \rppcu(n) = \left( n - \frac{i(1+i)}{2}, \frac{i(3+i)}{2} - n \right) \quad \text{ where } \quad i = \left\lfloor \frac{\sqrt{8n + 1} - 1}{2} \right\rfloor .\]
We want to implement $\rppcu$ inside $\RPP$, but there's a more natural way to define it than this.

Instead of thinking about $\rppcu$ analytically, let's imagine it again as a path in $\NN^2$:
we start at position $(0, 0)$ and successively apply a series of steps,
moving to $(x+1,y-1)$ if $y > 0$ or to $(0, x+1)$ if $y = 0$.
If we're able to find $\rppcustep \in \RPP$ such that
\[ \rppcustep(x,y) = \begin{cases} (x+1,y-1) &  y > 0 \\
                                   (0, x+1) &   y = 0 \end{cases} \]
then we can define $\rppcu_i = \rppIt[\rppcustep]$, so that $\rppcu_i(n, 0, 0)$ applies $n$ steps in the path, and reaches $(n, \rppcu(n))$
(the subscript $_i$ is motivated by the fact that we keep the input $n$).

It turns out that it's indeed possible to express $\rppcustep$ in $\RPP$:
\begin{lstlisting}
def cu_step :=
  Id₁ ‖ If Su Id₁ Id₁ ;;
  ⌊2, 0, 1⌉ ;;
  If (Su ‖ Pr) (Su ;; Sw) Id₁ ;;
  Sw ;; If Pr Id₁ Id₁ ;;
  Id₁ ‖ Sw
\end{lstlisting}

\makebox[\textwidth][c]{$\begin{NiceMatrix}
  \oloch{3-1}{\ y > 0} & x &                                                              & x & \bloch{3-1}{2 \\ 0 \\ 1} & 1 & \bloch{3-1}{\rppIf[\boldsymbol{\rppSu \rppPa \rppPr}, \ \rppSu \rppCo \rppSw, \ \rppId_1]} & 1     & \bloch{2-1}{\rppSw} & x + 1 & \bloch{2-1}{\rppIf[\boldsymbol{\rppPr}, \rppId_1, \rppId_1]} & x + 1 &                     & x + 1 \\
                       & y & \bloch{2-1}{\rppIf[\boldsymbol{\rppSu}, \rppId_1, \rppId_1]} & y &                          & x &                                                                                            & x + 1 &                     & 1     &                                                              & 0     & \bloch{2-1}{\rppSw} & y - 1 \\
                       & 0 &                                                              & 1 &                          & y &                                                                                            & y - 1 &                     & y - 1 &                                                              & y - 1 &                     & 0     \\
\\
  \oloch{3-1}{\ y = 0} & x &                                                              & x & \bloch{3-1}{2 \\ 0 \\ 1} & 0 & \bloch{3-1}{\rppIf[\rppSu \rppPa \rppPr, \ \boldsymbol{\rppSu \rppCo \rppSw}, \ \rppId_1]} & 0     & \bloch{2-1}{\rppSw} & 0     & \bloch{2-1}{\rppIf[\rppPr, \boldsymbol{\rppId_1}, \rppId_1]} & 0     &                     & 0     \\
                       & 0 & \bloch{2-1}{\rppIf[\rppSu, \boldsymbol{\rppId_1}, \rppId_1]} & 0 &                          & x &                                                                                            & 0     &                     & 0     &                                                              & 0     & \bloch{2-1}{\rppSw} & x + 1 \\
                       & 0 &                                                              & 0 &                          & 0 &                                                                                            & x + 1 &                     & x + 1 &                                                              & x + 1 &                     & 0     \\
\\
\end{NiceMatrix}$}

Notice that we can't directly use $y$ as the condition for the selection $\rppIf$,
because we also want to manipulate $y$ in those calculations.
We get around this limitation by using an ancillary variable initially set to $0$,
which assumes value $1$ or $0$ depending on whether $y>0$ or $y=0$,
and is then set back to $0$ using a variable that we know is positive or equal to zero depending on the case considered.

At this point we can define \lstinline{cuᵢ} (which also keeps the input around, hence the subscript) like so:
\begin{lstlisting}
def cuᵢ := It cu_step
\end{lstlisting}

\[\begin{NiceMatrix}
  n & \bloch{4-1}{\rppcu_i} & n \\
  0 &                       & x \\
  0 &                       & y \\
  0 &                       & 0 \\
\end{NiceMatrix}\]
where $(x,y)=\rppcu(n)$ or, equivalently, $n=\rppcp(x,y)$.

The function in the other direction is less intricate to define:
\begin{lstlisting}
def tr := It (Su ;; inc) -- triangular numbers
\end{lstlisting}
\[\begin{NiceMatrix}
  x & \bloch{3-1}{\rpptr} & x \\
  0 &                     & x \\
  0 &                     & \sum_{i=1}^x i \\
\end{NiceMatrix}\]
\begin{lstlisting}
def cpᵢ :=
  inc ;; Id₁ ‖ tr ;;
  Id₁ ‖ dec ;; dec ;;
  ⌊0, 3, 1⌉ ;; inc ;; Id₁ ‖ Sw 
\end{lstlisting}
\makebox[\textwidth][c]{$\begin{NiceMatrix}
  x & \bloch{2-1}{\rppinc} & x     &                     & x                    &                      & \bloch{2-1}{\rppdec} & x                    & \bloch{4-1}{0\\3\\1} & \bloch{2-1}{\rppinc} &                     & x                        & \Block{4-1}{=} & x           \\
  y &                      & x + y & \bloch{3-1}{\rpptr} & x + y                & \bloch{2-1}{\rppdec} &                      & y                    &                      &                      & \bloch{2-1}{\rppSw} & y                        &                & y           \\
  0 &                      & 0     &                     & x + y                &                      &                      & 0                    &                      &                      &                     & \sum_{i=1}^{x + y} i + x &                & \rppcp(x,y) \\
  0 &                      & 0     &                     & \sum_{i=1}^{x + y} i &                      &                      & \sum_{i=1}^{x + y} i &                      &                      &                     & 0                        &                & 0           \\
\\
\end{NiceMatrix}$}
We can combine \lstinline{cpᵢ} and \lstinline{cuᵢ} to get rid of the input values they both leave behind:
\begin{lstlisting}
def cp := cpᵢ ;; ⌊2, 0, 1⌉ ;; cuᵢ⁻¹
def cu := cp⁻¹
\end{lstlisting}
\[\begin{NiceMatrix}
  x & \bloch{4-1}{\rppcp_i} & x           & \bloch{3-1}{2\\0\\1} & \rppcp(x,y) & \bloch{4-1}{\rppcu_i^{-1}} & \rppcp(x,y) \\
  y &                       & y           &                      & x           &                            & 0           \\
  0 &                       & \rppcp(x,y) &                      & y           &                            & 0           \\
  0 &                       & 0           &                      & 0           &                            & 0           \\
\end{NiceMatrix}\]
This is very powerful: we succesfully defined a function which stores data from two positions to just one.
We will discuss pairings at greater lengths in the next paragraph; for now, let's focus on the technique we used.
In defining the function $\rppcu_i$ we framed it as a "path" in a two-dimensional grid.
By slightly tweaking $\rppcustep$ we can trace out other paths which helps us solve new problems.

For example, let's imagine starting on coordinates $(0,n)$ and moving each turn in direction $(+1,-1)$ (like before)
but now when we reach $(n,0)$, instead of jumping to $(0,n+1)$ we land again on $(0,n)$, looping in the same diagonal forever:

\begin{figure}[H]
  \centering
  \includegraphics[width=15em]{Immagini/div.png}
\end{figure}

If we call $\rppdivstep$ the function which performs one step of this diagram, by iterating it with $\rppIt[\rppdivstep]$
we've essentially found a way to do modular arithmetic: since it takes $n+1$ steps to get to where we started,
if we perform $m$ steps the $x$ coordinate will be such that $x \equiv m \pmod{n+1}$ and $0 \le x \le n$.
Furthermore, if we increase a counter by one each time we land on $(0,n)$ we can calculate division with remainder.
The differences between such a $\rppdivstep$ and $\rppcustep$ are minimal (highlighted in red):
\begin{lstlisting}
def div_step :=
  Id₁ ‖ If Su Id₁ Id₁ ;;
  ⌊2, 0, 1⌉ ;;
  If (Su ‖ Pr) @@(Sw ‖ Su)@@ Id₁ ;;
  Sw ;; If Pr Id₁ Id₁ ;;
  Id₁ ‖ Sw

def div := It div_step
\end{lstlisting}
The general behaviour of \lstinline{div} is the following: given \lstinline{n m : ℕ},
\begin{lstlisting}
‹div› [m, 0, n, 0, 0] = [m, r, (n+1) - r, 0, m / (n+1)]
\end{lstlisting}
where by \lstinline{/} is integer division and \lstinline{r = m % (n+1)} the remainder through the modulo operation.

Lastly, there's a way in which we can express the truncated square root function $\left\lfloor \sqrt{n} \right\rfloor$ in $\RPP$.
Of course, the square root is not an invertible function, like division - and like division,
we can get a "remainder" which is the difference $n - \left\lfloor \sqrt{n} \right\rfloor^2$.
We get $\rppsqrtstep$ by tweaking $\rppcustep$ a little - we start off at $(0,0)$ again,
but whenever we reach $(x,0)$ we jump to $(0,x+2)$ and increase a counter by one:

\begin{figure}[H]
  \centering
  \includegraphics[width=18em]{Immagini/sqrt.png}
\end{figure}

The first jump is performed $1$ step from the starting position $(0,0)$; the next one is performed after $3$ steps, then $5$, $7$ etc.
It's well known that for each $k$, the sum of the first $k$ odd numbers $1 + 3 + \dots + (2k - 1) = k^2$,
so when we reach the number $k^2$ we've performed $k$ jumps - hence, we calculated the square root.

Again, we highlight the (minor) differences with $\rppcustep$:
\begin{lstlisting}
def sqrt_step :=
  Id₁ ‖ If Su Id₁ Id₁ ;;
  ⌊2, 0, 1⌉ ;;
  If (Su ‖ Pr) @@(Su ;; Su ;; Sw ‖ Su)@@ Id₁ ;;
  Sw ;; If Pr Id₁ Id₁ ;;
  Id₁ ‖ Sw

def sqrt := It sqrt_step
\end{lstlisting}
We get this behaviour: given \lstinline{n : ℕ},
\begin{lstlisting}
‹sqrt› [n, 0, 0, 0, 0] = [n, r, 2 * √n - r, 0, √n]
\end{lstlisting}
where \lstinline{√n} is the truncated square root and \lstinline{r = n - √n * √n \ } our "square-root remainder".

\paragraph{Another pairing function}

\section{PRF-completeness}

\section{Alternative ways to define RPP in Lean}

\chapter{Conclusions}

\section{Future work}

\end{document}